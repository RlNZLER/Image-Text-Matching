{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import einops\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from official.nlp import optimization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for loading image and text data\n",
    "\n",
    "class ITM_DataLoader:\n",
    "    BATCH_SIZE = 16\n",
    "    IMAGE_SIZE = (224, 224)\n",
    "    IMAGE_SHAPE = (224, 224, 3)\n",
    "    MAX_SENTENCE_LENGTH = 50\n",
    "    SENTENCE_EMBEDDING_SHAPE = 384\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    DATA_PATH = \"D:\\_GITHUB_\\Image-Text-Matching\\data\"\n",
    "    IMAGES_PATH = DATA_PATH + \"/images\"\n",
    "    train_data_file = DATA_PATH + \"/flickr8k.TrainImages.txt\"\n",
    "    dev_data_file = DATA_PATH + \"/flickr8k.DevImages.txt\"\n",
    "    test_data_file = DATA_PATH + \"/flickr8k.TestImages.txt\"\n",
    "    sentence_embeddings_file = DATA_PATH + \"/flickr8k.cmp9137.sentence_transformers.pkl\"\n",
    "    sentence_embeddings = {}\n",
    "    train_ds = None\n",
    "    val_ds = None\n",
    "    test_ds = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sentence_embeddings = self.load_sentence_embeddings()\n",
    "        self.train_ds = self.load_classifier_data(self.train_data_file)\n",
    "        self.val_ds = self.load_classifier_data(self.dev_data_file)\n",
    "        self.test_ds = self.load_classifier_data(self.test_data_file)\n",
    "        print(\"done loading data...\")\n",
    "\n",
    "    # Sentence embeddings are dense vectors representing text data, one vector per sentence.\n",
    "    # Sentences with similar vectors would mean sentences with equivalent meanning.\n",
    "    # They are useful here to provide text-based features of questions in the data.\n",
    "    # Note: sentence embeddings don't include label info, they are solely based on captions.\n",
    "    def load_sentence_embeddings(self):\n",
    "        sentence_embeddings = {}\n",
    "        print(\"READING sentence embeddings...\")\n",
    "        with open(self.sentence_embeddings_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            for sentence, dense_vector in data.items():\n",
    "                # print(\"*sentence=\",sentence)\n",
    "                sentence_embeddings[sentence] = dense_vector\n",
    "        print(\"Done reading sentence_embeddings!\")\n",
    "        return sentence_embeddings\n",
    "\n",
    "    # In contrast to text-data based on pre-trained features, image data does not use\n",
    "    # any form of pre-training in this program. Instead, it makes use of raw pixels.\n",
    "    # Notes that input features to the classifier are only pixels and sentence embeddings.\n",
    "    def process_input(self, img_path, text_input_ids, text_attention_mask, label):\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, self.IMAGE_SIZE)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32) / 255.0\n",
    "        file_name = tf.strings.split(img_path, os.path.sep)[-1]\n",
    "        features = {\n",
    "            \"image_input\": img,\n",
    "            \"text_input_ids\": text_input_ids,\n",
    "            \"text_attention_mask\": text_attention_mask,\n",
    "            \"file_name\": file_name\n",
    "            }\n",
    "        return features, label\n",
    "\n",
    "    # This method loads the multimodal data, which comes from the following sources:\n",
    "    # (1) image files in IMAGES_PATH, and (2) files with pattern flickr8k.*Images.txt\n",
    "    # The data is stored in a tensorflow data structure to make it easy to use by\n",
    "    # the tensorflow model during training, validation and test. This method was\n",
    "    # carefully prepared to load the data rapidly, i.e., by loading already created\n",
    "    # sentence embeddings (text features) rather than creating them at runtime.\n",
    "    def load_classifier_data(self, data_files):\n",
    "        print(\"LOADING data from \" + str(data_files))\n",
    "        image_data = []\n",
    "        text_input_ids = []\n",
    "        text_attention_masks = []\n",
    "        label_data = []\n",
    "\n",
    "        with open(data_files) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.rstrip(\"\\n\")\n",
    "                img_name, text, raw_label = line.split(\"\\t\")\n",
    "                img_name = os.path.join(self.IMAGES_PATH, img_name.strip())\n",
    "                label = [1, 0] if raw_label == \"match\" else [0, 1]\n",
    "\n",
    "                encoding = self.tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.SENTENCE_EMBEDDING_SHAPE,  # Ensure this attribute is defined\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"tf\",\n",
    "                )\n",
    "\n",
    "                image_data.append(img_name)\n",
    "                text_input_ids.append(\n",
    "                    encoding[\"input_ids\"][0]\n",
    "                )  # [0] to unpack from batch\n",
    "                text_attention_masks.append(\n",
    "                    encoding[\"attention_mask\"][0]\n",
    "                )  # [0] to unpack from batch\n",
    "                label_data.append(label)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (image_data, text_input_ids, text_attention_masks, label_data)\n",
    "        )\n",
    "        dataset = dataset.map(self.process_input, num_parallel_calls=self.AUTOTUNE)\n",
    "        dataset = (\n",
    "            dataset.shuffle(self.BATCH_SIZE * 8)\n",
    "            .batch(self.BATCH_SIZE)\n",
    "            .prefetch(self.AUTOTUNE)\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    def print_data_samples(self, dataset):\n",
    "        print(\"PRINTING data samples...\")\n",
    "        print(\"-----------------------------------------\")\n",
    "        for features_batch, label_batch in dataset.take(1):\n",
    "            for i in range(1):\n",
    "                print(f'Image pixels: {features_batch[\"image_input\"]}')\n",
    "                print(f'Caption: {features_batch[\"text_input_ids\"].numpy()}')\n",
    "                label = label_batch.numpy()[i]\n",
    "                print(f\"Label : {label}\")\n",
    "        print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main class for the Image-Text Matching (ITM) task\n",
    "\n",
    "class ITM_Classifier(ITM_DataLoader):\n",
    "    epochs = 1\n",
    "    learning_rate = 4e-5\n",
    "    class_names = {\"match\", \"no-match\"}\n",
    "    num_classes = len(class_names)\n",
    "    classifier_model = None\n",
    "    history = None\n",
    "    classifier_model_name = \"ITM_Classifier-flickr\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.build_classifier_model()\n",
    "        self.train_classifier_model()\n",
    "        self.test_classifier_model()\n",
    "\n",
    "    # return learnt feature representations of input data (images)\n",
    "    def create_vision_encoder(\n",
    "        self, num_projection_layers, projection_dims, dropout_rate\n",
    "    ):\n",
    "        img_input = layers.Input(shape=self.IMAGE_SHAPE, name=\"image_input\")\n",
    "\n",
    "        # Use InceptionV3 with ImageNet weights, excluding the top (fully connected) layer\n",
    "        base_model = InceptionV3(\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\",\n",
    "            input_tensor=img_input,\n",
    "            input_shape=self.IMAGE_SHAPE,\n",
    "        )\n",
    "        # Avoid training the base model to retain learned features\n",
    "        base_model.trainable = False\n",
    "\n",
    "        # Continue from where the base model leaves off\n",
    "        cnn_layer = base_model.output\n",
    "        cnn_layer = layers.GlobalAveragePooling2D()(\n",
    "            cnn_layer\n",
    "        )  # Added to reduce dimensions\n",
    "        cnn_layer = layers.Dropout(dropout_rate)(cnn_layer)\n",
    "\n",
    "        outputs = self.project_embeddings(\n",
    "            cnn_layer, num_projection_layers, projection_dims, dropout_rate\n",
    "        )\n",
    "        return img_input, outputs\n",
    "\n",
    "    # return learnt feature representations based on dense layers, dropout, and layer normalisation\n",
    "    def project_embeddings(\n",
    "        self, embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    ):\n",
    "        projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "        for _ in range(num_projection_layers):\n",
    "            x = tf.nn.gelu(projected_embeddings)\n",
    "            x = layers.Dense(projection_dims)(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.Add()([projected_embeddings, x])\n",
    "            projected_embeddings = layers.LayerNormalization()(x)\n",
    "        return projected_embeddings\n",
    "\n",
    "    # return learnt feature representations of input data (text embeddings in the form of dense vectors)\n",
    "    def create_text_encoder(\n",
    "        self, num_projection_layers=1, projection_dims=128, dropout_rate=0.1\n",
    "    ):\n",
    "        bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        bert_model.trainable = (\n",
    "            False  # Set to False to freeze BERT weights, or True to fine-tune\n",
    "        )\n",
    "\n",
    "        # Define the inputs for the BERT model\n",
    "        text_input_ids = tf.keras.Input(\n",
    "            shape=(self.SENTENCE_EMBEDDING_SHAPE,),\n",
    "            dtype=tf.int32,\n",
    "            name=\"text_input_ids\",\n",
    "        )\n",
    "        text_attention_mask = tf.keras.Input(\n",
    "            shape=(self.SENTENCE_EMBEDDING_SHAPE,),\n",
    "            dtype=tf.int32,\n",
    "            name=\"text_attention_mask\",\n",
    "        )\n",
    "\n",
    "        # Getting the output from BERT\n",
    "        bert_output = bert_model(text_input_ids, attention_mask=text_attention_mask)\n",
    "        text_features = bert_output.last_hidden_state\n",
    "        text_features = tf.keras.layers.GlobalAveragePooling1D()(text_features)\n",
    "\n",
    "        # Project the BERT outputs to the desired dimensionality\n",
    "        projected_embeddings = tf.keras.layers.Dense(\n",
    "            projection_dims, activation=\"relu\"\n",
    "        )(text_features)\n",
    "        for _ in range(\n",
    "            1, num_projection_layers\n",
    "        ):  # start from 1 because we already added one Dense layer\n",
    "            x = tf.keras.layers.Dense(projection_dims, activation=\"relu\")(\n",
    "                projected_embeddings\n",
    "            )\n",
    "            x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "            projected_embeddings = tf.keras.layers.Add()(\n",
    "                [projected_embeddings, x]\n",
    "            )  # Element-wise addition\n",
    "            projected_embeddings = tf.keras.layers.LayerNormalization()(\n",
    "                projected_embeddings\n",
    "            )\n",
    "\n",
    "        return text_input_ids, text_attention_mask, projected_embeddings\n",
    "\n",
    "    # put together the feature representations above to create the image-text (multimodal) deep learning model\n",
    "    def build_classifier_model(self):\n",
    "        print(\"BUILDING model\")\n",
    "        # Create the vision model part\n",
    "        img_input, vision_net = self.create_vision_encoder(\n",
    "            num_projection_layers=1, projection_dims=128, dropout_rate=0.1\n",
    "        )\n",
    "\n",
    "        # Create the text model part\n",
    "        text_input_ids, text_attention_mask, text_net = self.create_text_encoder(\n",
    "            num_projection_layers=1, projection_dims=128, dropout_rate=0.1\n",
    "        )\n",
    "\n",
    "        # Combine the outputs from both text and vision parts\n",
    "        combined_features = tf.keras.layers.Concatenate(axis=1)([vision_net, text_net])\n",
    "        combined_features = tf.keras.layers.Dense(512, activation=\"relu\")(\n",
    "            combined_features\n",
    "        )\n",
    "        combined_features = tf.keras.layers.Dropout(0.1)(combined_features)\n",
    "        combined_features = tf.keras.layers.Dense(512, activation=\"relu\")(\n",
    "            combined_features\n",
    "        )\n",
    "        combined_features = tf.keras.layers.LayerNormalization()(combined_features)\n",
    "\n",
    "        # Classifier layer\n",
    "        final_output = tf.keras.layers.Dense(\n",
    "            self.num_classes, activation=\"softmax\", name=self.classifier_model_name\n",
    "        )(combined_features)\n",
    "\n",
    "        # Create the full model\n",
    "        self.classifier_model = tf.keras.Model(\n",
    "            inputs=[img_input, text_input_ids, text_attention_mask],\n",
    "            outputs=final_output,\n",
    "        )\n",
    "        self.classifier_model.summary()\n",
    "\n",
    "    def train_classifier_model(self):\n",
    "        print(f\"TRAINING model\")\n",
    "        steps_per_epoch = tf.data.experimental.cardinality(self.train_ds).numpy()\n",
    "        num_train_steps = steps_per_epoch * self.epochs\n",
    "        num_warmup_steps = int(0.2 * num_train_steps)\n",
    "\n",
    "        loss = tf.keras.losses.KLDivergence()\n",
    "        metrics = tf.keras.metrics.BinaryAccuracy()\n",
    "        optimizer = optimization.create_optimizer(\n",
    "            init_lr=self.learning_rate,\n",
    "            num_train_steps=num_train_steps,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            optimizer_type=\"adamw\",\n",
    "        )\n",
    "\n",
    "        self.classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "        # uncomment the next line if you wish to make use of early stopping during training\n",
    "        # callbacks = [tf.keras.callbacks.EarlyStopping(patience=11, restore_best_weights=True)]\n",
    "\n",
    "        self.history = self.classifier_model.fit(\n",
    "            x=self.train_ds, validation_data=self.val_ds, epochs=self.epochs\n",
    "        )  # , callbacks=callbacks)\n",
    "        print(\"model trained!\")\n",
    "\n",
    "    def test_classifier_model(self):\n",
    "        print(\n",
    "            \"TESTING classifier model (showing a sample of image-text-matching predictions)...\"\n",
    "        )\n",
    "        num_classifications = 0\n",
    "        num_correct_predictions = 0\n",
    "\n",
    "        # read test data for ITM classification\n",
    "        for features, groundtruth in self.test_ds:\n",
    "            groundtruth = groundtruth.numpy()\n",
    "            predictions = self.classifier_model(features)\n",
    "            predictions = predictions.numpy()\n",
    "            captions = features[\"text_input_ids\"].numpy()\n",
    "            file_names = features[\"file_name\"].numpy()\n",
    "\n",
    "            # read test data per batch\n",
    "            for batch_index in range(0, len(groundtruth)):\n",
    "                predicted_values = predictions[batch_index]\n",
    "                probability_match = predicted_values[0]\n",
    "                probability_nomatch = predicted_values[1]\n",
    "                predicted_class = (\n",
    "                    \"[1 0]\" if probability_match > probability_nomatch else \"[0 1]\"\n",
    "                )\n",
    "                if str(groundtruth[batch_index]) == predicted_class:\n",
    "                    num_correct_predictions += 1\n",
    "                num_classifications += 1\n",
    "\n",
    "                # print a sample of predictions -- about 10% of all possible\n",
    "                if random.random() < 0.1:\n",
    "                    caption = captions[batch_index]\n",
    "                    file_name = file_names[batch_index].decode(\"utf-8\")\n",
    "                    print(\n",
    "                        \"ITM=%s PREDICTIONS: match=%s, no-match=%s \\t -> \\t %s\"\n",
    "                        % (caption, probability_match, probability_nomatch, file_name)\n",
    "                    )\n",
    "\n",
    "        # reveal test performance using our own calculations above\n",
    "        accuracy = num_correct_predictions / num_classifications\n",
    "        print(\"TEST accuracy=%4f\" % (accuracy))\n",
    "\n",
    "        # reveal test performance using Tensorflow calculations\n",
    "        loss, accuracy = self.classifier_model.evaluate(self.test_ds)\n",
    "        print(f\"Tensorflow test method: Loss: {loss}; ACCURACY: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Let's create an instance of the main class\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING sentence embeddings...\n",
      "Done reading sentence_embeddings!\n",
      "LOADING data from D:\\_GITHUB_\\Image-Text-Matching\\data/flickr8k.TrainImages.txt\n",
      "LOADING data from D:\\_GITHUB_\\Image-Text-Matching\\data/flickr8k.DevImages.txt\n",
      "LOADING data from D:\\_GITHUB_\\Image-Text-Matching\\data/flickr8k.TestImages.txt\n",
      "done loading data...\n",
      "BUILDING model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 111, 111, 32  864         ['image_input[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 111, 111, 32  96         ['conv2d_94[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_94 (Activation)     (None, 111, 111, 32  0           ['batch_normalization_94[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)             (None, 109, 109, 32  9216        ['activation_94[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 109, 109, 32  96         ['conv2d_95[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_95 (Activation)     (None, 109, 109, 32  0           ['batch_normalization_95[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)             (None, 109, 109, 64  18432       ['activation_95[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 109, 109, 64  192        ['conv2d_96[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_96 (Activation)     (None, 109, 109, 64  0           ['batch_normalization_96[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 54, 54, 64)  0           ['activation_96[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)             (None, 54, 54, 80)   5120        ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 54, 54, 80)  240         ['conv2d_97[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_97 (Activation)     (None, 54, 54, 80)   0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)             (None, 52, 52, 192)  138240      ['activation_97[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_98 (BatchN  (None, 52, 52, 192)  576        ['conv2d_98[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_98 (Activation)     (None, 52, 52, 192)  0           ['batch_normalization_98[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 25, 25, 192)  0          ['activation_98[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)            (None, 25, 25, 64)   12288       ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 25, 25, 64)  192         ['conv2d_102[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_102 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)            (None, 25, 25, 48)   9216        ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_102[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 25, 25, 48)  144         ['conv2d_100[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_103 (Batch  (None, 25, 25, 96)  288         ['conv2d_103[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_100 (Activation)    (None, 25, 25, 48)   0           ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " activation_103 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_103[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_9 (AveragePo  (None, 25, 25, 192)  0          ['max_pooling2d_5[0][0]']        \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)             (None, 25, 25, 64)   12288       ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)            (None, 25, 25, 64)   76800       ['activation_100[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)            (None, 25, 25, 96)   82944       ['activation_103[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)            (None, 25, 25, 32)   6144        ['average_pooling2d_9[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_99 (BatchN  (None, 25, 25, 64)  192         ['conv2d_99[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 25, 25, 64)  192         ['conv2d_101[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_104 (Batch  (None, 25, 25, 96)  288         ['conv2d_104[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 25, 25, 32)  96          ['conv2d_105[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_99 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_99[0][0]'] \n",
      "                                                                                                  \n",
      " activation_101 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " activation_104 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_104[0][0]']\n",
      "                                                                                                  \n",
      " activation_105 (Activation)    (None, 25, 25, 32)   0           ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " mixed0 (Concatenate)           (None, 25, 25, 256)  0           ['activation_99[0][0]',          \n",
      "                                                                  'activation_101[0][0]',         \n",
      "                                                                  'activation_104[0][0]',         \n",
      "                                                                  'activation_105[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)            (None, 25, 25, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_109 (Batch  (None, 25, 25, 64)  192         ['conv2d_109[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_109 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_109[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)            (None, 25, 25, 48)   12288       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_109[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 25, 25, 48)  144         ['conv2d_107[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_110 (Batch  (None, 25, 25, 96)  288         ['conv2d_110[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_107 (Activation)    (None, 25, 25, 48)   0           ['batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " activation_110 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_110[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_10 (AverageP  (None, 25, 25, 256)  0          ['mixed0[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)            (None, 25, 25, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)            (None, 25, 25, 64)   76800       ['activation_107[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)            (None, 25, 25, 96)   82944       ['activation_110[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)            (None, 25, 25, 64)   16384       ['average_pooling2d_10[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 25, 25, 64)  192         ['conv2d_106[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_108 (Batch  (None, 25, 25, 64)  192         ['conv2d_108[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_111 (Batch  (None, 25, 25, 96)  288         ['conv2d_111[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_112 (Batch  (None, 25, 25, 64)  192         ['conv2d_112[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_106 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " activation_108 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_108[0][0]']\n",
      "                                                                                                  \n",
      " activation_111 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_111[0][0]']\n",
      "                                                                                                  \n",
      " activation_112 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_112[0][0]']\n",
      "                                                                                                  \n",
      " mixed1 (Concatenate)           (None, 25, 25, 288)  0           ['activation_106[0][0]',         \n",
      "                                                                  'activation_108[0][0]',         \n",
      "                                                                  'activation_111[0][0]',         \n",
      "                                                                  'activation_112[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_116 (Conv2D)            (None, 25, 25, 64)   18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 25, 25, 64)  192         ['conv2d_116[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_116 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_114 (Conv2D)            (None, 25, 25, 48)   13824       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_117 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_116[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_114 (Batch  (None, 25, 25, 48)  144         ['conv2d_114[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 25, 25, 96)  288         ['conv2d_117[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_114 (Activation)    (None, 25, 25, 48)   0           ['batch_normalization_114[0][0]']\n",
      "                                                                                                  \n",
      " activation_117 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_11 (AverageP  (None, 25, 25, 288)  0          ['mixed1[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)            (None, 25, 25, 64)   18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_115 (Conv2D)            (None, 25, 25, 64)   76800       ['activation_114[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_118 (Conv2D)            (None, 25, 25, 96)   82944       ['activation_117[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_119 (Conv2D)            (None, 25, 25, 64)   18432       ['average_pooling2d_11[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_113 (Batch  (None, 25, 25, 64)  192         ['conv2d_113[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 25, 25, 64)  192         ['conv2d_115[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_118 (Batch  (None, 25, 25, 96)  288         ['conv2d_118[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_119 (Batch  (None, 25, 25, 64)  192         ['conv2d_119[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_113 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_113[0][0]']\n",
      "                                                                                                  \n",
      " activation_115 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " activation_118 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_118[0][0]']\n",
      "                                                                                                  \n",
      " activation_119 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_119[0][0]']\n",
      "                                                                                                  \n",
      " mixed2 (Concatenate)           (None, 25, 25, 288)  0           ['activation_113[0][0]',         \n",
      "                                                                  'activation_115[0][0]',         \n",
      "                                                                  'activation_118[0][0]',         \n",
      "                                                                  'activation_119[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_121 (Conv2D)            (None, 25, 25, 64)   18432       ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 25, 25, 64)  192         ['conv2d_121[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_121 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_122 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_121[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 25, 25, 96)  288         ['conv2d_122[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_122 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_120 (Conv2D)            (None, 12, 12, 384)  995328      ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_123 (Conv2D)            (None, 12, 12, 96)   82944       ['activation_122[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 12, 12, 384)  1152       ['conv2d_120[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_123 (Batch  (None, 12, 12, 96)  288         ['conv2d_123[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_120 (Activation)    (None, 12, 12, 384)  0           ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " activation_123 (Activation)    (None, 12, 12, 96)   0           ['batch_normalization_123[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 12, 12, 288)  0          ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " mixed3 (Concatenate)           (None, 12, 12, 768)  0           ['activation_120[0][0]',         \n",
      "                                                                  'activation_123[0][0]',         \n",
      "                                                                  'max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_128 (Conv2D)            (None, 12, 12, 128)  98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_128 (Batch  (None, 12, 12, 128)  384        ['conv2d_128[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_128 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_128[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_129 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_128[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_129 (Batch  (None, 12, 12, 128)  384        ['conv2d_129[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_129 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_129[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_125 (Conv2D)            (None, 12, 12, 128)  98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_130 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_129[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 12, 12, 128)  384        ['conv2d_125[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_130 (Batch  (None, 12, 12, 128)  384        ['conv2d_130[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_125 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " activation_130 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_130[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_126 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_125[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_131 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_130[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_126 (Batch  (None, 12, 12, 128)  384        ['conv2d_126[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_131 (Batch  (None, 12, 12, 128)  384        ['conv2d_131[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_126 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_126[0][0]']\n",
      "                                                                                                  \n",
      " activation_131 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_12 (AverageP  (None, 12, 12, 768)  0          ['mixed3[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_124 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_127 (Conv2D)            (None, 12, 12, 192)  172032      ['activation_126[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_132 (Conv2D)            (None, 12, 12, 192)  172032      ['activation_131[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_133 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_12[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_124 (Batch  (None, 12, 12, 192)  576        ['conv2d_124[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_127 (Batch  (None, 12, 12, 192)  576        ['conv2d_127[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_132 (Batch  (None, 12, 12, 192)  576        ['conv2d_132[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_133 (Batch  (None, 12, 12, 192)  576        ['conv2d_133[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_124 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_124[0][0]']\n",
      "                                                                                                  \n",
      " activation_127 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_127[0][0]']\n",
      "                                                                                                  \n",
      " activation_132 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_132[0][0]']\n",
      "                                                                                                  \n",
      " activation_133 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_133[0][0]']\n",
      "                                                                                                  \n",
      " mixed4 (Concatenate)           (None, 12, 12, 768)  0           ['activation_124[0][0]',         \n",
      "                                                                  'activation_127[0][0]',         \n",
      "                                                                  'activation_132[0][0]',         \n",
      "                                                                  'activation_133[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_138 (Conv2D)            (None, 12, 12, 160)  122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_138 (Batch  (None, 12, 12, 160)  480        ['conv2d_138[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_138 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_138[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_139 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_138[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_139 (Batch  (None, 12, 12, 160)  480        ['conv2d_139[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_139 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_139[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_135 (Conv2D)            (None, 12, 12, 160)  122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_140 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_139[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_135 (Batch  (None, 12, 12, 160)  480        ['conv2d_135[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_140 (Batch  (None, 12, 12, 160)  480        ['conv2d_140[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_135 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_135[0][0]']\n",
      "                                                                                                  \n",
      " activation_140 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_140[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_136 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_135[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_141 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_140[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_136 (Batch  (None, 12, 12, 160)  480        ['conv2d_136[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_141 (Batch  (None, 12, 12, 160)  480        ['conv2d_141[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_136 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_136[0][0]']\n",
      "                                                                                                  \n",
      " activation_141 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_141[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_13 (AverageP  (None, 12, 12, 768)  0          ['mixed4[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_134 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_137 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_136[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_142 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_141[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_143 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_13[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_134 (Batch  (None, 12, 12, 192)  576        ['conv2d_134[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_137 (Batch  (None, 12, 12, 192)  576        ['conv2d_137[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_142 (Batch  (None, 12, 12, 192)  576        ['conv2d_142[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_143 (Batch  (None, 12, 12, 192)  576        ['conv2d_143[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_134 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_134[0][0]']\n",
      "                                                                                                  \n",
      " activation_137 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_137[0][0]']\n",
      "                                                                                                  \n",
      " activation_142 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_142[0][0]']\n",
      "                                                                                                  \n",
      " activation_143 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_143[0][0]']\n",
      "                                                                                                  \n",
      " mixed5 (Concatenate)           (None, 12, 12, 768)  0           ['activation_134[0][0]',         \n",
      "                                                                  'activation_137[0][0]',         \n",
      "                                                                  'activation_142[0][0]',         \n",
      "                                                                  'activation_143[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_148 (Conv2D)            (None, 12, 12, 160)  122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_148 (Batch  (None, 12, 12, 160)  480        ['conv2d_148[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_148 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_148[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_149 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_148[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_149 (Batch  (None, 12, 12, 160)  480        ['conv2d_149[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_149 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_149[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_145 (Conv2D)            (None, 12, 12, 160)  122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_150 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_149[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_145 (Batch  (None, 12, 12, 160)  480        ['conv2d_145[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_150 (Batch  (None, 12, 12, 160)  480        ['conv2d_150[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_145 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_145[0][0]']\n",
      "                                                                                                  \n",
      " activation_150 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_150[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_146 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_145[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_151 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_150[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_146 (Batch  (None, 12, 12, 160)  480        ['conv2d_146[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_151 (Batch  (None, 12, 12, 160)  480        ['conv2d_151[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_146 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_146[0][0]']\n",
      "                                                                                                  \n",
      " activation_151 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_151[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_14 (AverageP  (None, 12, 12, 768)  0          ['mixed5[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_144 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_147 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_146[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_152 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_151[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_153 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_14[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_144 (Batch  (None, 12, 12, 192)  576        ['conv2d_144[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_147 (Batch  (None, 12, 12, 192)  576        ['conv2d_147[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_152 (Batch  (None, 12, 12, 192)  576        ['conv2d_152[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_153 (Batch  (None, 12, 12, 192)  576        ['conv2d_153[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_144 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_144[0][0]']\n",
      "                                                                                                  \n",
      " activation_147 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_147[0][0]']\n",
      "                                                                                                  \n",
      " activation_152 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_152[0][0]']\n",
      "                                                                                                  \n",
      " activation_153 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_153[0][0]']\n",
      "                                                                                                  \n",
      " mixed6 (Concatenate)           (None, 12, 12, 768)  0           ['activation_144[0][0]',         \n",
      "                                                                  'activation_147[0][0]',         \n",
      "                                                                  'activation_152[0][0]',         \n",
      "                                                                  'activation_153[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_158 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_158 (Batch  (None, 12, 12, 192)  576        ['conv2d_158[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_158 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_158[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_159 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_158[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_159 (Batch  (None, 12, 12, 192)  576        ['conv2d_159[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_159 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_159[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_155 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_160 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_159[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_155 (Batch  (None, 12, 12, 192)  576        ['conv2d_155[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 12, 12, 192)  576        ['conv2d_160[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_155 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_155[0][0]']\n",
      "                                                                                                  \n",
      " activation_160 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_156 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_155[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_161 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_160[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_156 (Batch  (None, 12, 12, 192)  576        ['conv2d_156[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_161 (Batch  (None, 12, 12, 192)  576        ['conv2d_161[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_156 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_156[0][0]']\n",
      "                                                                                                  \n",
      " activation_161 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_15 (AverageP  (None, 12, 12, 768)  0          ['mixed6[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_154 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_157 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_156[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_162 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_161[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_163 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_15[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_154 (Batch  (None, 12, 12, 192)  576        ['conv2d_154[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_157 (Batch  (None, 12, 12, 192)  576        ['conv2d_157[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, 12, 12, 192)  576        ['conv2d_162[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_163 (Batch  (None, 12, 12, 192)  576        ['conv2d_163[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_154 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_154[0][0]']\n",
      "                                                                                                  \n",
      " activation_157 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_157[0][0]']\n",
      "                                                                                                  \n",
      " activation_162 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " activation_163 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_163[0][0]']\n",
      "                                                                                                  \n",
      " mixed7 (Concatenate)           (None, 12, 12, 768)  0           ['activation_154[0][0]',         \n",
      "                                                                  'activation_157[0][0]',         \n",
      "                                                                  'activation_162[0][0]',         \n",
      "                                                                  'activation_163[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_166 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 12, 12, 192)  576        ['conv2d_166[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_166 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_167 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_166[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 12, 12, 192)  576        ['conv2d_167[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_167 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_164 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_168 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_167[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_164 (Batch  (None, 12, 12, 192)  576        ['conv2d_164[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_168 (Batch  (None, 12, 12, 192)  576        ['conv2d_168[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_164 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_164[0][0]']\n",
      "                                                                                                  \n",
      " activation_168 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_168[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_165 (Conv2D)            (None, 5, 5, 320)    552960      ['activation_164[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_169 (Conv2D)            (None, 5, 5, 192)    331776      ['activation_168[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 5, 5, 320)   960         ['conv2d_165[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_169 (Batch  (None, 5, 5, 192)   576         ['conv2d_169[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_165 (Activation)    (None, 5, 5, 320)    0           ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " activation_169 (Activation)    (None, 5, 5, 192)    0           ['batch_normalization_169[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 5, 5, 768)   0           ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " mixed8 (Concatenate)           (None, 5, 5, 1280)   0           ['activation_165[0][0]',         \n",
      "                                                                  'activation_169[0][0]',         \n",
      "                                                                  'max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_174 (Conv2D)            (None, 5, 5, 448)    573440      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_174 (Batch  (None, 5, 5, 448)   1344        ['conv2d_174[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_174 (Activation)    (None, 5, 5, 448)    0           ['batch_normalization_174[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_171 (Conv2D)            (None, 5, 5, 384)    491520      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_175 (Conv2D)            (None, 5, 5, 384)    1548288     ['activation_174[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_171 (Batch  (None, 5, 5, 384)   1152        ['conv2d_171[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_175 (Batch  (None, 5, 5, 384)   1152        ['conv2d_175[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_171 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_171[0][0]']\n",
      "                                                                                                  \n",
      " activation_175 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_175[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_172 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_171[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_173 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_171[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_176 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_175[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_177 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_175[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling2d_16 (AverageP  (None, 5, 5, 1280)  0           ['mixed8[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_170 (Conv2D)            (None, 5, 5, 320)    409600      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_172 (Batch  (None, 5, 5, 384)   1152        ['conv2d_172[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_173 (Batch  (None, 5, 5, 384)   1152        ['conv2d_173[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_176 (Batch  (None, 5, 5, 384)   1152        ['conv2d_176[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_177 (Batch  (None, 5, 5, 384)   1152        ['conv2d_177[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_178 (Conv2D)            (None, 5, 5, 192)    245760      ['average_pooling2d_16[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_170 (Batch  (None, 5, 5, 320)   960         ['conv2d_170[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_172 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " activation_173 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_173[0][0]']\n",
      "                                                                                                  \n",
      " activation_176 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_176[0][0]']\n",
      "                                                                                                  \n",
      " activation_177 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_177[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_178 (Batch  (None, 5, 5, 192)   576         ['conv2d_178[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_170 (Activation)    (None, 5, 5, 320)    0           ['batch_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " mixed9_0 (Concatenate)         (None, 5, 5, 768)    0           ['activation_172[0][0]',         \n",
      "                                                                  'activation_173[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 5, 5, 768)    0           ['activation_176[0][0]',         \n",
      "                                                                  'activation_177[0][0]']         \n",
      "                                                                                                  \n",
      " activation_178 (Activation)    (None, 5, 5, 192)    0           ['batch_normalization_178[0][0]']\n",
      "                                                                                                  \n",
      " mixed9 (Concatenate)           (None, 5, 5, 2048)   0           ['activation_170[0][0]',         \n",
      "                                                                  'mixed9_0[0][0]',               \n",
      "                                                                  'concatenate_3[0][0]',          \n",
      "                                                                  'activation_178[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_183 (Conv2D)            (None, 5, 5, 448)    917504      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_183 (Batch  (None, 5, 5, 448)   1344        ['conv2d_183[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_183 (Activation)    (None, 5, 5, 448)    0           ['batch_normalization_183[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_180 (Conv2D)            (None, 5, 5, 384)    786432      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_184 (Conv2D)            (None, 5, 5, 384)    1548288     ['activation_183[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_180 (Batch  (None, 5, 5, 384)   1152        ['conv2d_180[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_184 (Batch  (None, 5, 5, 384)   1152        ['conv2d_184[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_180 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_180[0][0]']\n",
      "                                                                                                  \n",
      " activation_184 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_184[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_181 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_180[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_182 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_180[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_185 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_184[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_186 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_184[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling2d_17 (AverageP  (None, 5, 5, 2048)  0           ['mixed9[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_179 (Conv2D)            (None, 5, 5, 320)    655360      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_181 (Batch  (None, 5, 5, 384)   1152        ['conv2d_181[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_182 (Batch  (None, 5, 5, 384)   1152        ['conv2d_182[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_185 (Batch  (None, 5, 5, 384)   1152        ['conv2d_185[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_186 (Batch  (None, 5, 5, 384)   1152        ['conv2d_186[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_187 (Conv2D)            (None, 5, 5, 192)    393216      ['average_pooling2d_17[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_179 (Batch  (None, 5, 5, 320)   960         ['conv2d_179[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_181 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_181[0][0]']\n",
      "                                                                                                  \n",
      " activation_182 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_182[0][0]']\n",
      "                                                                                                  \n",
      " activation_185 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_185[0][0]']\n",
      "                                                                                                  \n",
      " activation_186 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_186[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_187 (Batch  (None, 5, 5, 192)   576         ['conv2d_187[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_179 (Activation)    (None, 5, 5, 320)    0           ['batch_normalization_179[0][0]']\n",
      "                                                                                                  \n",
      " mixed9_1 (Concatenate)         (None, 5, 5, 768)    0           ['activation_181[0][0]',         \n",
      "                                                                  'activation_182[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 5, 5, 768)    0           ['activation_185[0][0]',         \n",
      "                                                                  'activation_186[0][0]']         \n",
      "                                                                                                  \n",
      " activation_187 (Activation)    (None, 5, 5, 192)    0           ['batch_normalization_187[0][0]']\n",
      "                                                                                                  \n",
      " mixed10 (Concatenate)          (None, 5, 5, 2048)   0           ['activation_179[0][0]',         \n",
      "                                                                  'mixed9_1[0][0]',               \n",
      "                                                                  'concatenate_4[0][0]',          \n",
      "                                                                  'activation_187[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 2048)        0           ['mixed10[0][0]']                \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_40 (Dropout)           (None, 2048)         0           ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          262272      ['dropout_40[0][0]']             \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 128)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          16512       ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " text_input_ids (InputLayer)    [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " text_attention_mask (InputLaye  [(None, 384)]       0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 128)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['text_input_ids[0][0]',         \n",
      "                                thPoolingAndCrossAt               'text_attention_mask[0][0]']    \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 384,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 128)          0           ['dense_5[0][0]',                \n",
      "                                                                  'dropout_41[0][0]']             \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 768)         0           ['tf_bert_model_1[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 128)         256         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 128)          98432       ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 256)          0           ['layer_normalization_2[0][0]',  \n",
      "                                                                  'dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 512)          131584      ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_79 (Dropout)           (None, 512)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 512)          262656      ['dropout_79[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 512)         1024        ['dense_9[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " ITM_Classifier-flickr (Dense)  (None, 2)            1026        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 132,058,786\n",
      "Trainable params: 773,762\n",
      "Non-trainable params: 131,285,024\n",
      "__________________________________________________________________________________________________\n",
      "TRAINING model\n",
      "1212/1212 [==============================] - 876s 707ms/step - loss: 0.7218 - binary_accuracy: 0.5561 - val_loss: 0.6530 - val_binary_accuracy: 0.6237\n",
      "model trained!\n",
      "TESTING classifier model (showing a sample of image-text-matching predictions)...\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"self\" \"                 f\"(type TFBertSelfAttention).\n\n{{function_node __wrapped__Softmax_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[16,12,384,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Softmax]\n\nCall arguments received by layer \"self\" \"                 f\"(type TFBertSelfAttention):\n  • hidden_states=tf.Tensor(shape=(16, 384, 768), dtype=float32)\n  • attention_mask=tf.Tensor(shape=(16, 1, 1, 384), dtype=float32)\n  • head_mask=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_value=None\n  • output_attentions=False\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m itm \u001b[38;5;241m=\u001b[39m \u001b[43mITM_Classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m, in \u001b[0;36mITM_Classifier.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_classifier_model()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_classifier_model()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_classifier_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 176\u001b[0m, in \u001b[0;36mITM_Classifier.test_classifier_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, groundtruth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_ds:\n\u001b[0;32m    175\u001b[0m     groundtruth \u001b[38;5;241m=\u001b[39m groundtruth\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m--> 176\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    178\u001b[0m     captions \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32md:\\Conda\\envs\\GPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Conda\\envs\\GPU\\lib\\site-packages\\transformers\\modeling_tf_utils.py:409\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    408\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32md:\\Conda\\envs\\GPU\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:1108\u001b[0m, in \u001b[0;36mTFBertModel.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1087\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;124;03m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;124;03m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;124;03m        `past_key_values`). Set to `False` during training, `True` during generation\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1108\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32md:\\Conda\\envs\\GPU\\lib\\site-packages\\transformers\\modeling_tf_utils.py:409\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    408\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32md:\\Conda\\envs\\GPU\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:863\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m     head_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers\n\u001b[1;32m--> 863\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    877\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    878\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(hidden_states\u001b[38;5;241m=\u001b[39msequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Conda\\envs\\GPU\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:554\u001b[0m, in \u001b[0;36mTFBertEncoder.call\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    550\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m    552\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 554\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32md:\\Conda\\envs\\GPU\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:464\u001b[0m, in \u001b[0;36mTFBertLayer.call\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    453\u001b[0m     hidden_states: tf\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32md:\\Conda\\envs\\GPU\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:380\u001b[0m, in \u001b[0;36mTFBertAttention.call\u001b[1;34m(self, input_tensor, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    371\u001b[0m     input_tensor: tf\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     training: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    379\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 380\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_output(\n\u001b[0;32m    391\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mself_outputs[\u001b[38;5;241m0\u001b[39m], input_tensor\u001b[38;5;241m=\u001b[39minput_tensor, training\u001b[38;5;241m=\u001b[39mtraining\n\u001b[0;32m    392\u001b[0m     )\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;66;03m# add attentions (possibly with past_key_value) if we output them\u001b[39;00m\n",
      "File \u001b[1;32md:\\Conda\\envs\\GPU\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:319\u001b[0m, in \u001b[0;36mTFBertSelfAttention.call\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[0;32m    316\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39madd(attention_scores, attention_mask)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mstable_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(inputs\u001b[38;5;241m=\u001b[39mattention_probs, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[1;32md:\\Conda\\envs\\GPU\\lib\\site-packages\\transformers\\tf_utils.py:70\u001b[0m, in \u001b[0;36mstable_softmax\u001b[1;34m(logits, axis, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03mStable wrapper that returns the same output as `tf.nn.softmax`, but that works reliably with XLA on CPU. It is\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03mmeant as a workaround for the [following issue](https://github.com/tensorflow/tensorflow/issues/55682), and will be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m        A Tensor. Has the same type and shape as logits.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# TODO: When the issue linked above gets sorted, add a check on TF version here and use the original function if\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# it has the fix. After we drop the support for unfixed versions, remove this function.\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"self\" \"                 f\"(type TFBertSelfAttention).\n\n{{function_node __wrapped__Softmax_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[16,12,384,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Softmax]\n\nCall arguments received by layer \"self\" \"                 f\"(type TFBertSelfAttention):\n  • hidden_states=tf.Tensor(shape=(16, 384, 768), dtype=float32)\n  • attention_mask=tf.Tensor(shape=(16, 1, 1, 384), dtype=float32)\n  • head_mask=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_value=None\n  • output_attentions=False\n  • training=False"
     ]
    }
   ],
   "source": [
    "itm = ITM_Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'itm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mitm\u001b[49m\u001b[38;5;241m.\u001b[39mtest_classifier_model()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'itm' is not defined"
     ]
    }
   ],
   "source": [
    "itm.test_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    acc = history.history['binary_accuracy']\n",
    "    val_acc = history.history['val_binary_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'b-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r-', label='Validation accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(itm.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
