{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Text Matching Classifier: baseline system\n",
    "\n",
    "This program has been adapted and rewritten from sources such as:\n",
    "- [TensorFlow CNN Tutorial](https://www.tensorflow.org/tutorials/images/cnn)\n",
    "- [Keras Concatenate Layer Documentation](https://keras.io/api/layers/merging_layers/concatenate/)\n",
    "- [Sentence Transformers Package](https://pypi.org/project/sentence-transformers/)\n",
    "\n",
    "If you are new to TensorFlow, read the following brief tutorial:\n",
    "- [TensorFlow Quickstart Beginner Tutorial](https://www.tensorflow.org/tutorials/quickstart/beginner)\n",
    "\n",
    "As you develop your experience and skills, you may want to check details of particular aspects of the TensorFlow API:\n",
    "- [TensorFlow API Documentation](https://www.tensorflow.org/api_docs/python/tf/all_symbols)\n",
    "\n",
    "This is a binary classifier for image-text matching, where the inputs are images and text-based features, and the outputs (denoted as match=[1,0] and nomatch=[0,1]) correspond to (predicted) answers. This baseline classifier makes use of two strands of features. The first are produced by a CNN-classifier, and the second are derived offline from a sentence embedding generator. The latter have the advantage of being generated once, which can accelerate training due to being pre-trained and loaded at runtime. Those two strands of features are concatenated at training time to form a multimodal set of features, combining learnt image features and pre-trained sentence features.\n",
    "\n",
    "This program has been tested using an Anaconda environment with Python 3.9 and 3.10 on Windows 11 and Linux Ubuntu 22. The easiest way to run this baseline at Uni is by booting your PC with Windows and using the following steps:\n",
    "\n",
    "1. Make sure that your downloaded data and baseline system are extracted in the Downloads folder. Note. Your path should start with /mnt/c/Users/Computing/Downloads\n",
    "\n",
    "2. Open a terminal and select Ubuntu from the little arrow pointing down. Note. Your program will be executed under a Linux environment.\n",
    "\n",
    "3. Install the following dependencies:\n",
    "\n",
    "4. Edit file ITM_Classifier-baseline.py and make sure that variable IMAGES_PATH points to the right folder containing the data.\n",
    "\n",
    "5. Run the program using a command such as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ python ITM_Classifier-baseline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The code above can also be run from Visual Studio Code. To access it using the Linux environment type \"code .\" in the Ubuntu terminal. From VSCode, click View, Terminal, type your command (example: python ITM_Classifier-baseline.py) and Enter.\n",
    "\n",
    "Running this baseline takes about 5 minutes with a GPU-enabled Uni PC. WARNING: Running this code without a GPU is too slow and not recommended.\n",
    "\n",
    "In your own PC you can use Anaconda to run this code. From a conda terminal for example. If you want GPU-enabled execution, it is recommended that you install the following versions of software:\n",
    "- CUDA 11.8\n",
    "- CuDNN 8.6\n",
    "- TensorFlow 2.10\n",
    "\n",
    "Feel free to use and/or modify this program as part of your CMP9137 assignment. You are invited to use the knowledge acquired during lectures, workshops and beyond to propose and evaluate alternative solutions to this baseline.\n",
    "\n",
    "Version 1.0, main functionality tested with COCO data\n",
    "Version 1.2, extended functionality for Flickr data\n",
    "Contact: {hcuayahuitl, lzhang, friaz}@lincoln.ac.uk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the dependencies\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import einops\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from official.nlp import optimization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n",
      "No GPU detected. Running on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Verify TensorFlow can detect the GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f\"Num GPUs Available: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "if len(gpus) > 0:\n",
    "    print(f\"Using GPU: {gpus[0].device_type} {gpus[0].name}\")\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for loading image and text data\n",
    "\n",
    "class ITM_DataLoader():\n",
    "    BATCH_SIZE = 16\n",
    "    IMAGE_SIZE = (224, 224)\n",
    "    IMAGE_SHAPE = (224, 224, 3)\n",
    "    SENTENCE_EMBEDDING_SHAPE = (384)\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    IMAGES_PATH = \"data/images\"\n",
    "    train_data_file = IMAGES_PATH+\"/../flickr8k.TrainImages.txt\"\n",
    "    dev_data_file = IMAGES_PATH+\"/../flickr8k.DevImages.txt\"\n",
    "    test_data_file = IMAGES_PATH+\"/../flickr8k.TestImages.txt\"\n",
    "    sentence_embeddings_file = IMAGES_PATH+\"/../flickr8k.cmp9137.sentence_transformers.pkl\"\n",
    "    sentence_embeddings = {}\n",
    "    train_ds = None\n",
    "    val_ds = None\n",
    "    test_ds = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sentence_embeddings = self.load_sentence_embeddings()\n",
    "        self.train_ds = self.load_classifier_data(self.train_data_file)\n",
    "        self.val_ds = self.load_classifier_data(self.dev_data_file)\n",
    "        self.test_ds = self.load_classifier_data(self.test_data_file)\n",
    "        print(\"done loading data...\")\n",
    "\n",
    "    # Sentence embeddings are dense vectors representing text data, one vector per sentence. \n",
    "    # Sentences with similar vectors would mean sentences with equivalent meanning.  \n",
    "\t# They are useful here to provide text-based features of questions in the data.\n",
    "    # Note: sentence embeddings don't include label info, they are solely based on captions.\n",
    "    def load_sentence_embeddings(self):\n",
    "        sentence_embeddings = {}\n",
    "        print(\"READING sentence embeddings...\")\n",
    "        with open(self.sentence_embeddings_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            for sentence, dense_vector in data.items():\n",
    "                sentence_embeddings[sentence] = dense_vector\n",
    "                #print(\"*sentence=\",sentence)\n",
    "        print(\"Done reading sentence_embeddings!\")\n",
    "        return sentence_embeddings\n",
    "\n",
    "    # In contrast to text-data based on pre-trained features, image data does not use\n",
    "    # any form of pre-training in this program. Instead, it makes use of raw pixels.\n",
    "    # Notes that input features to the classifier are only pixels and sentence embeddings.\n",
    "    def process_input(self, img_path, dense_vector, text, label):\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, self.IMAGE_SIZE)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.cast(img, tf.float32) / 255\n",
    "        features = {}\n",
    "        features[\"image_input\"] = img\n",
    "        features[\"text_embedding\"] = dense_vector\n",
    "        features[\"caption\"] = text\n",
    "        features[\"file_name\"] = img_path\n",
    "        return features, label\n",
    "\n",
    "    # This method loads the multimodal data, which comes from the following sources:\n",
    "    # (1) image files in IMAGES_PATH, and (2) files with pattern flickr8k.*Images.txt\n",
    "    # The data is stored in a tensorflow data structure to make it easy to use by\n",
    "    # the tensorflow model during training, validation and test. This method was \n",
    "    # carefully prepared to load the data rapidly, i.e., by loading already created\n",
    "    # sentence embeddings (text features) rather than creating them at runtime.\n",
    "    def load_classifier_data(self, data_files):\n",
    "        print(\"LOADING data from \"+str(data_files))\n",
    "        print(\"=========================================\")\n",
    "        image_data = []\n",
    "        text_data = []\n",
    "        embeddings_data = []\n",
    "        label_data = []\n",
    "\t\t\n",
    "        # get image, text, label of image_files\n",
    "        with open(data_files) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.rstrip(\"\\n\")\n",
    "                img_name, text, raw_label = line.split(\"\t\")\n",
    "                img_name = os.path.join(self.IMAGES_PATH, img_name.strip())\n",
    "\n",
    "                # get binary labels from match/no-match answers\n",
    "                label = [1, 0] if raw_label == \"match\" else [0, 1]\n",
    "                #print(\"I=%s T=%s _L=%s L=%s\" % (img_name, text, raw_label, label)) \n",
    "\n",
    "\t\t\t\t# get sentence embeddings (of textual captions)\n",
    "                text_sentence_embedding = self.sentence_embeddings[text]\n",
    "                text_sentence_embedding = tf.constant(text_sentence_embedding)\n",
    "\n",
    "                image_data.append(img_name)\n",
    "                embeddings_data.append(text_sentence_embedding)\n",
    "                text_data.append(text)\n",
    "                label_data.append(label)\n",
    "\n",
    "        print(\"|image_data|=\"+str(len(image_data)))\n",
    "        print(\"|text_data|=\"+str(len(text_data)))\n",
    "        print(\"|label_data|=\"+str(len(label_data)))\n",
    "\t\t\n",
    "        # prepare a tensorflow dataset using the lists generated above\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_data, embeddings_data, text_data, label_data))\n",
    "        dataset = dataset.shuffle(self.BATCH_SIZE * 8)\n",
    "        dataset = dataset.map(self.process_input, num_parallel_calls=self.AUTOTUNE)\n",
    "        dataset = dataset.batch(self.BATCH_SIZE).prefetch(self.AUTOTUNE)\n",
    "        self.print_data_samples(dataset)\n",
    "        return dataset\n",
    "\n",
    "    def print_data_samples(self, dataset):\n",
    "        print(\"PRINTING data samples...\")\n",
    "        print(\"-----------------------------------------\")\n",
    "        for features_batch, label_batch in dataset.take(1):\n",
    "            for i in range(1):\n",
    "                print(f'Image pixels: {features_batch[\"image_input\"]}')\n",
    "                print(f'Sentence embeddings: {features_batch[\"text_embedding\"]}')\n",
    "                print(f'Caption: {features_batch[\"caption\"].numpy()}')\n",
    "                label = label_batch.numpy()[i]\n",
    "                print(f'Label : {label}')\n",
    "        print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main class for the Image-Text Matching (ITM) task\n",
    "\n",
    "class ITM_Classifier(ITM_DataLoader):\n",
    "    epochs = 10\n",
    "    learning_rate = 3e-5\n",
    "    class_names = {'match', 'no-match'}\n",
    "    num_classes = len(class_names)\n",
    "    classifier_model = None\n",
    "    history = None\n",
    "    classifier_model_name = 'ITM_Classifier-flickr'\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.build_classifier_model()\n",
    "        self.train_classifier_model()\n",
    "        self.test_classifier_model()\n",
    "\n",
    "    # return learnt feature representations of input data (images)\n",
    "    def create_vision_encoder(self, num_projection_layers, projection_dims, dropout_rate):\n",
    "        img_input = layers.Input(shape=self.IMAGE_SHAPE, name=\"image_input\")\n",
    "        cnn_layer = layers.Conv2D(16, 3, padding='same', activation='relu')(img_input)\n",
    "        cnn_layer = layers.MaxPooling2D()(cnn_layer)\n",
    "        cnn_layer = layers.Conv2D(32, 3, padding='same', activation='relu')(cnn_layer)\n",
    "        cnn_layer = layers.MaxPooling2D()(cnn_layer)\n",
    "        cnn_layer = layers.Conv2D(64, 3, padding='same', activation='relu')(cnn_layer)\n",
    "        cnn_layer = layers.MaxPooling2D()(cnn_layer)\n",
    "        cnn_layer = layers.Dropout(dropout_rate)(cnn_layer)\n",
    "        cnn_layer = layers.Flatten()(cnn_layer)\n",
    "        outputs = self.project_embeddings(cnn_layer, num_projection_layers, projection_dims, dropout_rate)\n",
    "        return img_input, outputs\n",
    "\n",
    "    # return learnt feature representations based on dense layers, dropout, and layer normalisation\n",
    "    def project_embeddings(self, embeddings, num_projection_layers, projection_dims, dropout_rate):\n",
    "        projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "        for _ in range(num_projection_layers):\n",
    "            x = tf.nn.gelu(projected_embeddings)\n",
    "            x = layers.Dense(projection_dims)(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.Add()([projected_embeddings, x])\n",
    "            projected_embeddings = layers.LayerNormalization()(x)\n",
    "        return projected_embeddings\n",
    "\n",
    "    # return learnt feature representations of input data (text embeddings in the form of dense vectors)\n",
    "    def create_text_encoder(self, num_projection_layers, projection_dims, dropout_rate):\n",
    "        text_input = keras.Input(shape=self.SENTENCE_EMBEDDING_SHAPE, name='text_embedding')\n",
    "        outputs = self.project_embeddings(text_input, num_projection_layers, projection_dims, dropout_rate)\n",
    "        return text_input, outputs\n",
    "\n",
    "    # put together the feature representations above to create the image-text (multimodal) deep learning model\n",
    "    def build_classifier_model(self):\n",
    "        print(f'BUILDING model')\n",
    "        img_input, vision_net = self.create_vision_encoder(num_projection_layers=1, projection_dims=128, dropout_rate=0.1)\n",
    "        text_input, text_net = self.create_text_encoder(num_projection_layers=1, projection_dims=128, dropout_rate=0.1)\n",
    "        net = tf.keras.layers.Concatenate(axis=1)([vision_net, text_net])\n",
    "        net = tf.keras.layers.Dropout(0.1)(net)\n",
    "        net = tf.keras.layers.Dense(self.num_classes, activation='softmax', name=self.classifier_model_name)(net)\n",
    "        self.classifier_model = tf.keras.Model(inputs=[img_input, text_input], outputs=net)\n",
    "        self.classifier_model.summary()\n",
    "\t\n",
    "    def train_classifier_model(self):\n",
    "        print(f'TRAINING model')\n",
    "        steps_per_epoch = tf.data.experimental.cardinality(self.train_ds).numpy()\n",
    "        num_train_steps = steps_per_epoch * self.epochs\n",
    "        num_warmup_steps = int(0.2*num_train_steps)\n",
    "\n",
    "        loss = tf.keras.losses.KLDivergence()\n",
    "        metrics = tf.keras.metrics.BinaryAccuracy()\n",
    "        optimizer = optimization.create_optimizer(init_lr=self.learning_rate,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "\n",
    "        self.classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "        # uncomment the next line if you wish to make use of early stopping during training\n",
    "        #callbacks = [tf.keras.callbacks.EarlyStopping(patience=11, restore_best_weights=True)]\n",
    "\n",
    "        self.history = self.classifier_model.fit(x=self.train_ds, validation_data=self.val_ds, epochs=self.epochs)#, callbacks=callbacks)\n",
    "        print(\"model trained!\")\n",
    "\n",
    "    def test_classifier_model(self):\n",
    "        print(\"TESTING classifier model (showing a sample of image-text-matching predictions)...\")\n",
    "        num_classifications = 0\n",
    "        num_correct_predictions = 0\n",
    "\n",
    "        # read test data for ITM classification\n",
    "        for features, groundtruth in self.test_ds:\n",
    "            groundtruth = groundtruth.numpy()\n",
    "            predictions = self.classifier_model(features)\n",
    "            predictions = predictions.numpy()\n",
    "            captions = features[\"caption\"].numpy()\n",
    "            file_names = features[\"file_name\"].numpy()\n",
    "\n",
    "            # read test data per batch\n",
    "            for batch_index in range(0, len(groundtruth)):\n",
    "                predicted_values = predictions[batch_index]\n",
    "                probability_match = predicted_values[0]\n",
    "                probability_nomatch = predicted_values[1]\n",
    "                predicted_class = \"[1 0]\" if probability_match > probability_nomatch else \"[0 1]\"\n",
    "                if str(groundtruth[batch_index]) == predicted_class: \n",
    "                    num_correct_predictions += 1\n",
    "                num_classifications += 1\n",
    "\n",
    "                # print a sample of predictions -- about 10% of all possible\n",
    "                if random.random() < 0.1:\n",
    "                    caption = captions[batch_index]\n",
    "                    file_name = file_names[batch_index].decode(\"utf-8\")\n",
    "                    print(\"ITM=%s PREDICTIONS: match=%s, no-match=%s \\t -> \\t %s\" % (caption, probability_match, probability_nomatch, file_name))\n",
    "\n",
    "        # reveal test performance using our own calculations above\n",
    "        accuracy = num_correct_predictions/num_classifications\n",
    "        print(\"TEST accuracy=%4f\" % (accuracy))\n",
    "\n",
    "        # reveal test performance using Tensorflow calculations\n",
    "        loss, accuracy = self.classifier_model.evaluate(self.test_ds)\n",
    "        print(f'Tensorflow test method: Loss: {loss}; ACCURACY: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING sentence embeddings...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/images/../flickr8k.cmp9137.sentence_transformers.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let's create an instance of the main class\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m itm \u001b[38;5;241m=\u001b[39m \u001b[43mITM_Classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mITM_Classifier.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_classifier_model()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_classifier_model()\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mITM_DataLoader.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_sentence_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m#Loading Sentence embeddings in dictionary format.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_classifier_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data_file)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_classifier_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdev_data_file)\n",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36mITM_DataLoader.load_sentence_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m sentence_embeddings \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREADING sentence embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_embeddings_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     32\u001b[0m     data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence, dense_vector \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Github/Image-Text-Matching/env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/images/../flickr8k.cmp9137.sentence_transformers.pkl'"
     ]
    }
   ],
   "source": [
    "# Let's create an instance of the main class\n",
    "\n",
    "itm = ITM_Classifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
