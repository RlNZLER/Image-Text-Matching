{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install einops\n",
    "# !pip install tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have the GPU enabled if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/content/drive/MyDrive/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading Class\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, data_file, images_path, tokenizer, transform=None):\n",
    "        self.data_frame = pd.read_csv(data_file, sep=\"\\t\", names=[\"image\", \"text\", \"label\"])\n",
    "        self.images_path = images_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_frame.iloc[idx]\n",
    "        image_path = os.path.join(self.images_path, row['image'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        text = row['text']\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        label = torch.tensor(1 if row['label'] == 'match' else 0, dtype=torch.float)\n",
    "\n",
    "        return image, inputs.input_ids.squeeze(0), inputs.attention_mask.squeeze(0), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Class\n",
    "class ImageTextMatchingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageTextMatchingModel, self).__init__()\n",
    "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.vision_encoder = models.resnet50(pretrained=True)\n",
    "        # Replace the classifier of ResNet50\n",
    "        num_ftrs = self.vision_encoder.fc.in_features\n",
    "        self.vision_encoder.fc = nn.Linear(num_ftrs, 512)\n",
    "\n",
    "        # Classifier to combine vision and text features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 + self.text_encoder.config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        text_features = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        vision_features = self.vision_encoder(images)\n",
    "        combined_features = torch.cat((vision_features, text_features), dim=1)\n",
    "        logits = self.classifier(combined_features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "IMAGES_PATH = DATA_PATH + \"/images\"\n",
    "train_data_file = DATA_PATH + '/flickr8k.TrainImages.txt'\n",
    "dev_data_file = DATA_PATH + '/flickr8k.DevImages.txt'\n",
    "test_data_file = DATA_PATH + '/flickr8k.TestImages.txt'\n",
    "\n",
    "# Assume you have defined your dataset paths: train_data_file, dev_data_file, test_data_file\n",
    "train_dataset = Flickr8kDataset(train_data_file, IMAGES_PATH, tokenizer, transform)\n",
    "val_dataset = Flickr8kDataset(dev_data_file, IMAGES_PATH, tokenizer, transform)\n",
    "test_dataset = Flickr8kDataset(test_data_file, IMAGES_PATH, tokenizer, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, num_workers=2)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = ImageTextMatchingModel().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}', leave=True)\n",
    "        for i, (images, input_ids, attention_mask, labels) in progress_bar:\n",
    "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, input_ids, attention_mask).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': running_loss/(i+1)})\n",
    "        \n",
    "        evaluate_model(model, val_loader)\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    predictions, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(loader, desc='Evaluating', leave=False)\n",
    "        for images, input_ids, attention_mask, labels in progress_bar:\n",
    "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(images, input_ids, attention_mask).squeeze()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            truths.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(truths, predictions)\n",
    "    print(f'Validation Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function calls\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    acc = history.history['binary_accuracy']\n",
    "    val_acc = history.history['val_binary_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'b-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r-', label='Validation accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(itm.history)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
