{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preparing the Dataset\n",
    "We'll create a custom PyTorch dataset class. Ensure your dataset file (e.g., a CSV or JSON file) includes the image paths, captions, and match labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rinzler/Github/Image-Text-Matching/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor\n",
    "from transformers import BertTokenizer\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, image_dir='data/images', image_size=(224, 224)):\n",
    "        \"\"\"\n",
    "        dataframe: A pandas DataFrame containing 'image_path', 'caption', and 'label'\n",
    "        tokenizer: An instance of BertTokenizer\n",
    "        image_dir: Directory where images are stored (this parameter might be unused if image paths in dataframe are absolute)\n",
    "        image_size: A tuple indicating the size to which images are resized\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = Compose([\n",
    "            Resize(image_size),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_path = row['image_path']\n",
    "        caption = row['caption']\n",
    "        label = row['label']\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')  # Make sure images are in RGB\n",
    "        image = self.image_transform(image)\n",
    "        \n",
    "        text = self.tokenizer(caption, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        label = torch.tensor(label).long()\n",
    "        \n",
    "        return image, text, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Model Definition\n",
    "We combine BERT for text encoding and a CNN (e.g., ResNet) for image encoding. Then, we'll merge their outputs to predict the match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageTextMatchingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageTextMatchingModel, self).__init__()\n",
    "        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.image_model = resnet50(pretrained=True)\n",
    "        self.image_model.fc = nn.Linear(self.image_model.fc.in_features, 768) # Matching BERT's embedding size\n",
    "        self.classifier = nn.Linear(768 * 2, 2) # Binary classification\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        image_features = self.image_model(images)\n",
    "        combined_features = torch.cat((text_features, image_features), dim=1)\n",
    "        logits = self.classifier(combined_features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training Loop\n",
    "For simplicity, the following is a very basic training loop. You'll need to add data loading, model instantiation, an optimizer, and a loss function. Also, consider adding validation and testing phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, epochs=5, device='cpu'):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for images, texts, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            input_ids = texts['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = texts['attention_mask'].squeeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training\n",
    "\n",
    "CUDA: If you have a GPU, make sure to move your model and data batches to CUDA to accelerate training. You can do this by calling .to('cuda') on your model and each batch of data and labels in your training loop.\n",
    "\n",
    "Validation and Testing: Similar steps should be followed for validation and testing, where you'd evaluate the model's performance on unseen data and adjust hyperparameters accordingly.\n",
    "\n",
    "Saving and Loading Models: Consider saving your trained model periodically or after training completes so you can reload it later for inference or continue training. Use torch.save(model.state_dict(), 'model_path.pth') to save and model.load_state_dict(torch.load('model_path.pth')) to load.\n",
    "\n",
    "Fine-Tuning and Hyperparameters: The initial learning rate, batch size, and the number of epochs are starting points. Depending on your dataset size and specific task, you may need to fine-tune these and other hyperparameters.\n",
    "\n",
    "Dataset Splitting: The provided code doesn't explicitly split the dataset into training, validation, and test sets. For a thorough evaluation, you should split your data accordingly and evaluate your model on the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rinzler/Github/Image-Text-Matching/env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rinzler/Github/Image-Text-Matching/env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Assuming your data file and images directory are correctly set\n",
    "train_file_path = '/home/rinzler/Github/Image-Text-Matching/data/flickr8k.TrainImages.txt'\n",
    "images_directory = '/home/rinzler/Github/Image-Text-Matching/data/images'\n",
    "\n",
    "\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Read your dataset (adjust as needed)\n",
    "data = []\n",
    "with open(train_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 3:\n",
    "            data.append({\n",
    "                'image_path': os.path.join(images_directory, parts[0]),\n",
    "                'caption': parts[1],\n",
    "                'label': 1 if parts[2] == 'match' else 0\n",
    "            })\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create the DataFrame from your data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = ImageTextDataset(df, tokenizer, image_dir=images_directory)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "# Model and move it to the device (GPU/CPU)\n",
    "model = ImageTextMatchingModel().to(device)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model, dataloader, optimizer, criterion, epochs=5, device=device)\n",
    "\n",
    "model_save_path = 'trained_models/image_text_matching_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
