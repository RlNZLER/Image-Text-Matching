{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 19:00:38.028103: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-29 19:00:38.119266: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 19:00:38.142288: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 19:00:38.531673: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-29 19:00:38.531722: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-29 19:00:38.531726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/computing/anaconda3/envs/AML/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/computing/anaconda3/envs/AML/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/computing/anaconda3/xyzzz/Task1/models\")\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from official.nlp import optimization\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, TFBertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class for loading image and text data\n",
    "\n",
    "\n",
    "class ITM_DataLoader():\n",
    "    BATCH_SIZE = 16\n",
    "    IMAGE_SIZE = (224, 224)\n",
    "    IMAGE_SHAPE = (224, 224, 3)\n",
    "    SENTENCE_EMBEDDING_SHAPE = (384)\n",
    "    max_sentence_length = 300\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    IMAGES_PATH = \"./flickr8k-resised\"\n",
    "    train_data_file = \"./flickr8k.TrainImages.txt\"\n",
    "    dev_data_file = \"./flickr8k.DevImages.txt\"\n",
    "    test_data_file = \"./flickr8k.TestImages.txt\"\n",
    "    sentence_embeddings_file = \"./flickr8k.cmp9137.sentence_transformers.pkl\"\n",
    "    sentence_embeddings = {}\n",
    "    train_ds = None\n",
    "    val_ds = None\n",
    "    test_ds = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sentence_embeddings = self.load_sentence_embeddings()\n",
    "        self.train_ds = self.load_classifier_data(self.train_data_file)\n",
    "        self.val_ds = self.load_classifier_data(self.dev_data_file)\n",
    "        self.test_ds = self.load_classifier_data(self.test_data_file)\n",
    "        print(\"done loading data...\")\n",
    "\n",
    "    # Sentence embeddings are dense vectors representing text data, one vector per sentence.\n",
    "    # Sentences with similar vectors would mean sentences with equivalent meanning.\n",
    "        # They are useful here to provide text-based features of questions in the data.\n",
    "    # Note: sentence embeddings don't include label info, they are solely based on captions.\n",
    "    def load_sentence_embeddings(self):\n",
    "        sentence_embeddings = {}\n",
    "        print(\"READING sentence embeddings...\")\n",
    "        with open(self.sentence_embeddings_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            for sentence, dense_vector in data.items():\n",
    "                # print(\"*sentence=\",sentence)\n",
    "                sentence_embeddings[sentence] = dense_vector\n",
    "        print(\"Done reading sentence_embeddings!\")\n",
    "        return sentence_embeddings\n",
    "\n",
    "    def padded_tensor(self, unpadded_tensor):\n",
    "        # max_length = 250max(len(tensor[0]) for tensor in unpadded_tensor)\n",
    "        print(self.max_sentence_length)\n",
    "        padded = []\n",
    "        for tensor in unpadded_tensor:\n",
    "            padding_length = self.max_sentence_length - len(tensor[0])\n",
    "            padded_tensor = tf.pad(tensor[0], paddings=[[0, padding_length]], constant_values=0)\n",
    "            reshaped_tensor = tf.reshape(padded_tensor, (1, self.max_sentence_length))\n",
    "            padded.append(reshaped_tensor)\n",
    "\n",
    "        return padded\n",
    "\n",
    "    # In contrast to text-data based on pre-trained features, image data does not use\n",
    "    # any form of pre-training in this program. Instead, it makes use of raw pixels.\n",
    "    # Notes that input features to the classifier are only pixels and sentence embeddings.\n",
    "    def process_input(self, img_path, text, label, text_input_ids, text_attention_mask):\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, self.IMAGE_SIZE)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.cast(img, tf.float32) / 255\n",
    "        # print('text', text)\n",
    "        # print('img_path', img_path)\n",
    "        # print('bert_input_ids', bert_input_ids)\n",
    "        # print('bert_attention_mask', bert_attention_mask)\n",
    "        features = {}\n",
    "        features[\"image_input\"] = img\n",
    "        features[\"text_input_ids\"] = text_input_ids[0]\n",
    "        features['text_attention_mask'] = text_attention_mask[0]\n",
    "        features[\"caption\"] = text\n",
    "        features[\"file_name\"] = img_path\n",
    "        return features, label\n",
    "\n",
    "    # This method loads the multimodal data, which comes from the following sources:\n",
    "    # (1) image files in IMAGES_PATH, and (2) files with pattern flickr8k.*Images.txt\n",
    "    # The data is stored in a tensorflow data structure to make it easy to use by\n",
    "    # the tensorflow model during training, validation and test. This method was\n",
    "    # carefully prepared to load the data rapidly, i.e., by loading already created\n",
    "    # sentence embeddings (text features) rather than creating them at runtime.\n",
    "    def load_classifier_data(self, data_files):\n",
    "        print(\"LOADING data from \"+str(data_files))\n",
    "        print(\"=========================================\")\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        image_data = []\n",
    "        text_data = []\n",
    "        label_data = []\n",
    "        bert_input_ids = []\n",
    "        bert_attention_mask = []\n",
    "\n",
    "        # Tokenize the text data\n",
    "        # get image, text, label of image_files\n",
    "        with open(data_files) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.rstrip(\"\\n\")\n",
    "                img_name, text, raw_label = line.split(\"\t\")\n",
    "                img_name = os.path.join(self.IMAGES_PATH, img_name.strip())\n",
    "\n",
    "                # get binary labels from match/no-match answers\n",
    "                label = [1, 0] if raw_label == \"match\" else [0, 1]\n",
    "                # print(\"I=%s T=%s _L=%s L=%s\" % (img_name, text, raw_label, label))\n",
    "\n",
    "                # get sentence embeddings (of textual captions)\n",
    "                # text_sentence_embedding = self.sentence_embeddings[text]\n",
    "                # text_sentence_embedding = tf.constant(text_sentence_embedding)\n",
    "\n",
    "                image_data.append(img_name)\n",
    "                # embeddings_data.append(text_sentence_embedding)\n",
    "                text_data.append(text)\n",
    "                label_data.append(label)\n",
    "                tokenized_texts = tokenizer(\n",
    "                    text, padding=True, truncation=True, return_tensors='tf')\n",
    "                bert_input_ids.append(tokenized_texts['input_ids'])\n",
    "                bert_attention_mask.append(\n",
    "                    tokenized_texts['attention_mask'])\n",
    "\n",
    "        \n",
    "        bert_input_ids = self.padded_tensor(bert_input_ids)\n",
    "        bert_attention_mask = self.padded_tensor(bert_attention_mask)\n",
    "        print(\"|image_data|=\"+str(len(image_data)), image_data[0])\n",
    "        print(\"|text_data|=\"+str(len(text_data)), text_data[0])\n",
    "        print(\"|label_data|=\"+str(len(label_data)), label_data[0])\n",
    "        # print(\"|bert_input_ids|=\"+str(len(bert_input_ids)), bert_input_ids[0])\n",
    "        # print(\"|bert_attention_mask|=\" +\n",
    "            #   str(len(bert_attention_mask)), bert_attention_mask[0])\n",
    "\n",
    "        # prepare a tensorflow dataset using the lists generated above\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (image_data, text_data, label_data, bert_input_ids, bert_attention_mask))\n",
    "        dataset = dataset.shuffle(self.BATCH_SIZE * 8)\n",
    "        dataset = dataset.map(self.process_input,\n",
    "                              num_parallel_calls=self.AUTOTUNE)\n",
    "        dataset = dataset.batch(self.BATCH_SIZE).prefetch(self.AUTOTUNE)\n",
    "        # self.print_data_samples(dataset)\n",
    "        return dataset\n",
    "\n",
    "    def print_data_samples(self, dataset):\n",
    "        print(\"PRINTING data samples...\")\n",
    "        print(\"-----------------------------------------\")\n",
    "        for features_batch, label_batch in dataset.take(1):\n",
    "            for i in range(1):\n",
    "                print(f'Image pixels: {features_batch[\"image_input\"]}')\n",
    "                print(\n",
    "                    f'Sentence embeddings: {features_batch[\"text_embedding\"]}')\n",
    "                print(f'Caption: {features_batch[\"caption\"].numpy()}')\n",
    "                label = label_batch.numpy()[i]\n",
    "                print(f'Label : {label}')\n",
    "        print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ITM_Classifier(ITM_DataLoader):\n",
    "    epochs = 10\n",
    "    learning_rate = 4e-5\n",
    "    class_names = {'match', 'no-match'}\n",
    "    num_classes = len(class_names)\n",
    "    classifier_model = None\n",
    "    history = None\n",
    "    classifier_model_name = 'ITM_Classifier-flickr'\n",
    "    best_model_name = \"ITM_Best_model.keras\"\n",
    "    best_model = None\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.build_classifier_model()\n",
    "        self.train_classifier_model()\n",
    "        self.test_classifier_model()\n",
    "\n",
    "    # return learnt feature representations of input data (images)\n",
    "    def create_vision_encoder(self, num_projection_layers, projection_dims, dropout_rate):\n",
    "        img_input = layers.Input(shape=self.IMAGE_SHAPE, name=\"image_input\")\n",
    "        cnn_layer = layers.Conv2D(32, 3, padding='same')(img_input)\n",
    "        cnn_layer = layers.BatchNormalization()(cnn_layer)\n",
    "        cnn_layer = layers.Activation(\"relu\")(cnn_layer)\n",
    "        cnn_layer = layers.MaxPooling2D()(cnn_layer)\n",
    "        cnn_layer = layers.Conv2D(64, 3, padding='same')(cnn_layer)\n",
    "        cnn_layer = layers.BatchNormalization()(cnn_layer)\n",
    "        cnn_layer = layers.Activation(\"relu\")(cnn_layer)\n",
    "        cnn_layer = layers.MaxPooling2D()(cnn_layer)\n",
    "        cnn_layer = layers.Conv2D(128, 3, padding='same')(cnn_layer)\n",
    "        cnn_layer = layers.BatchNormalization()(cnn_layer)\n",
    "        cnn_layer = layers.Activation(\"relu\")(cnn_layer)\n",
    "        cnn_layer = layers.MaxPooling2D()(cnn_layer)\n",
    "        cnn_layer = layers.Conv2D(256, 3, padding='same')(cnn_layer)\n",
    "        cnn_layer = layers.BatchNormalization()(cnn_layer)\n",
    "        cnn_layer = layers.Activation(\"relu\")(cnn_layer)\n",
    "        cnn_layer = layers.MaxPooling2D()(cnn_layer)\n",
    "        cnn_layer = layers.Dropout(dropout_rate)(cnn_layer)\n",
    "        cnn_layer = layers.Flatten()(cnn_layer)\n",
    "        outputs = self.project_embeddings(\n",
    "            cnn_layer, num_projection_layers, projection_dims, dropout_rate)\n",
    "        return img_input, outputs\n",
    "\n",
    "    # return learnt feature representations based on dense layers, dropout, and layer normalisation\n",
    "    def project_embeddings(self, embeddings, num_projection_layers, projection_dims, dropout_rate):\n",
    "        projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "        for _ in range(num_projection_layers):\n",
    "            x = tf.nn.relu(projected_embeddings)\n",
    "            x = layers.Dense(projection_dims, activation='relu')(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            # x = layers.Add()([projected_embeddings, x])\n",
    "            projected_embeddings = layers.LayerNormalization()(x)\n",
    "        return projected_embeddings\n",
    "\n",
    "    # return learnt feature representations of input data (text embeddings in the form of dense vectors)\n",
    "    def create_text_encoder(self, num_projection_layers, projection_dims, dropout_rate):\n",
    "        bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "        for layer in bert_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        max_sequence_length = 300\n",
    "        text_input_ids = tf.keras.Input(\n",
    "            shape=(max_sequence_length,), dtype=tf.int32, name='text_input_ids')\n",
    "        text_attention_mask = tf.keras.Input(\n",
    "            shape=(max_sequence_length,), dtype=tf.int32, name='text_attention_mask')\n",
    "        text_output = bert_model(\n",
    "            text_input_ids, attention_mask=text_attention_mask)\n",
    "        # Extract hidden states as features\n",
    "        text_features = text_output.last_hidden_state\n",
    "        text_features = tf.keras.layers.GlobalAveragePooling1D()(text_features)\n",
    "        text_features = self.project_embeddings(\n",
    "            text_features, num_projection_layers, projection_dims, dropout_rate)\n",
    "        return text_input_ids, text_attention_mask, text_features  # ,tokenizer\n",
    "        text_input = keras.Input(\n",
    "            shape=self.SENTENCE_EMBEDDING_SHAPE, name='text_embedding')\n",
    "        outputs = self.project_embeddings(\n",
    "            text_input, num_projection_layers, projection_dims, dropout_rate)\n",
    "        return text_input, outputs\n",
    "\n",
    "    # put together the feature representations above to create the image-text (multimodal) deep learning model\n",
    "    def build_classifier_model(self):\n",
    "        print(f'BUILDING model')\n",
    "        img_input, vision_net = self.create_vision_encoder(\n",
    "            num_projection_layers=1, projection_dims=512, dropout_rate=0.1)\n",
    "        \n",
    "        text_input_ids, text_attention_mask, text_net = self.create_text_encoder(\n",
    "            num_projection_layers=2, projection_dims=512, dropout_rate=0.1)\n",
    "        net = tf.keras.layers.Concatenate(axis=1)([vision_net, text_net])\n",
    "        net = tf.keras.layers.Dropout(0.1)(net)\n",
    "\n",
    "        combined_features = tf.keras.layers.Dense(512, activation='relu')(net)\n",
    "        combined_features = tf.keras.layers.LayerNormalization()(combined_features)\n",
    "        combined_features = tf.keras.layers.Dropout(0.1)(combined_features)        \n",
    "        combined_features = tf.keras.layers.Dense(512, activation='relu')(combined_features)\n",
    "        combined_features = tf.keras.layers.LayerNormalization()(combined_features)\n",
    "        combined_features = tf.keras.layers.Dropout(0.1)(combined_features)\n",
    "        # combined_features = tf.keras.layers.Dense(512, activation='relu')(combined_features)\n",
    "        # combined_features = tf.keras.layers.LayerNormalization()(combined_features)\n",
    "        # net = tf.keras.layers.Dropout(0.1)(combined_features)\n",
    "\n",
    "        # net = tf.keras.layers.Dense(512, activation='relu')(net)\n",
    "        # net = tf.keras.layers.Dropout(0.1)(net)\n",
    "        # net = tf.keras.layers.Dense(512, activation='relu')(net)\n",
    "        # net = tf.keras.layers.Dropout(0.1)(net)\n",
    "\n",
    "        net = tf.keras.layers.Dense(\n",
    "            self.num_classes, activation='softmax', name=self.classifier_model_name)(net)\n",
    "\n",
    "        self.classifier_model = tf.keras.Model(\n",
    "            inputs=[img_input, text_input_ids, text_attention_mask], outputs=net)\n",
    "        self.classifier_model.summary()\n",
    "\n",
    "    def train_classifier_model(self):\n",
    "        print(f'TRAINING model')\n",
    "        steps_per_epoch = tf.data.experimental.cardinality(\n",
    "            self.train_ds).numpy()\n",
    "        num_train_steps = steps_per_epoch * self.epochs\n",
    "        num_warmup_steps = int(0.2*num_train_steps)\n",
    "\n",
    "        loss = tf.keras.losses.KLDivergence()\n",
    "        metrics = tf.keras.metrics.BinaryAccuracy()\n",
    "        optimizer = optimization.create_optimizer(init_lr=self.learning_rate,\n",
    "                                                  num_train_steps=num_train_steps,\n",
    "                                                  num_warmup_steps=num_warmup_steps,\n",
    "                                                  optimizer_type='adamw')\n",
    "\n",
    "        self.classifier_model.compile(\n",
    "            optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        print(self.classifier_model.summary())\n",
    "\n",
    "        # uncomment the next line if you wish to make use of early stopping during training\n",
    "        # callbacks = [tf.keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True)]\n",
    "        # callbacks = [tf.keras.callbacks.ModelCheckpoint(self.best_model_name, save_best_only=True)]\n",
    "\n",
    "        self.history = self.classifier_model.fit(\n",
    "            x=self.train_ds, validation_data=self.val_ds, epochs=self.epochs, batch_size=128)  # , callbacks=callbacks)\n",
    "        print(\"model trained!\")\n",
    "\n",
    "    def test_classifier_model(self):\n",
    "        # self.best_model =  tf.keras.models.load_model(self.best_model_name)\n",
    "        print(\"TESTING classifier model (showing a sample of image-text-matching predictions)...\")\n",
    "        num_classifications = 0\n",
    "        num_correct_predictions = 0\n",
    "\n",
    "        # read test data for ITM classification\n",
    "        for features, groundtruth in self.test_ds:\n",
    "            groundtruth = groundtruth.numpy()\n",
    "            predictions = self.classifier_model(features)\n",
    "            # predictions = self.best_model(features)\n",
    "            predictions = predictions.numpy()\n",
    "            captions = features[\"caption\"].numpy()\n",
    "            file_names = features[\"file_name\"].numpy()\n",
    "\n",
    "            # read test data per batch\n",
    "            for batch_index in range(0, len(groundtruth)):\n",
    "                predicted_values = predictions[batch_index]\n",
    "                probability_match = predicted_values[0]\n",
    "                probability_nomatch = predicted_values[1]\n",
    "                predicted_class = \"[1 0]\" if probability_match > probability_nomatch else \"[0 1]\"\n",
    "                if str(groundtruth[batch_index]) == predicted_class:\n",
    "                    num_correct_predictions += 1\n",
    "                num_classifications += 1\n",
    "\n",
    "                # print a sample of predictions -- about 10% of all possible\n",
    "                if random.random() < 0.1:\n",
    "                    caption = captions[batch_index]\n",
    "                    file_name = file_names[batch_index].decode(\"utf-8\")\n",
    "                    print(\"ITM=%s PREDICTIONS: match=%s, no-match=%s \\t -> \\t %s\" %\n",
    "                          (caption, probability_match, probability_nomatch, file_name))\n",
    "\n",
    "        # reveal test performance using our own calculations above\n",
    "        accuracy = num_correct_predictions/num_classifications\n",
    "        print(\"TEST accuracy=%4f\" % (accuracy))\n",
    "\n",
    "        # reveal test performance using Tensorflow calculations\n",
    "        loss, accuracy = self.classifier_model.evaluate(self.test_ds)\n",
    "        # loss, accuracy = self.best_model.evaluate(self.test_ds)\n",
    "        print(f'Tensorflow test method: Loss: {loss}; ACCURACY: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING sentence embeddings...\n",
      "Done reading sentence_embeddings!\n",
      "LOADING data from ./flickr8k.TrainImages.txt\n",
      "=========================================\n",
      "300\n",
      "300\n",
      "|image_data|=19386 ./flickr8k-resised/1001773457_577c3a7d70.jpg\n",
      "|text_data|=19386 A black dog and a spotted dog are fighting\n",
      "|label_data|=19386 [1, 0]\n",
      "LOADING data from ./flickr8k.DevImages.txt\n",
      "=========================================\n",
      "300\n",
      "300\n",
      "|image_data|=1164 ./flickr8k-resised/103205630_682ca7285b.jpg\n",
      "|text_data|=1164 two men setting up a blue ice fishing hut on an iced over lake\n",
      "|label_data|=1164 [1, 0]\n",
      "LOADING data from ./flickr8k.TestImages.txt\n",
      "=========================================\n",
      "300\n",
      "300\n",
      "|image_data|=1161 ./flickr8k-resised/101669240_b2d3e7f17b.jpg\n",
      "|text_data|=1161 A skier looks at framed pictures in the snow next to trees .\n",
      "|label_data|=1161 [1, 0]\n",
      "done loading data...\n",
      "BUILDING model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 224, 224, 32  896         ['image_input[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 224, 224, 32  128        ['conv2d_16[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 224, 224, 32  0           ['batch_normalization_16[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_16 (MaxPooling2D  (None, 112, 112, 32  0          ['activation_16[0][0]']          \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 112, 112, 64  18496       ['max_pooling2d_16[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 112, 112, 64  256        ['conv2d_17[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 112, 112, 64  0           ['batch_normalization_17[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_17 (MaxPooling2D  (None, 56, 56, 64)  0           ['activation_17[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 56, 56, 128)  73856       ['max_pooling2d_17[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 56, 56, 128)  512        ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 56, 56, 128)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_18 (MaxPooling2D  (None, 28, 28, 128)  0          ['activation_18[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " text_input_ids (InputLayer)    [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " text_attention_mask (InputLaye  [(None, 300)]       0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 28, 28, 256)  295168      ['max_pooling2d_18[0][0]']       \n",
      "                                                                                                  \n",
      " tf_bert_model_4 (TFBertModel)  TFBaseModelOutputWi  109482240   ['text_input_ids[0][0]',         \n",
      "                                thPoolingAndCrossAt               'text_attention_mask[0][0]']    \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 300,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 28, 28, 256)  1024       ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4 (Gl  (None, 768)         0           ['tf_bert_model_4[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 28, 28, 256)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 512)          393728      ['global_average_pooling1d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " max_pooling2d_19 (MaxPooling2D  (None, 14, 14, 256)  0          ['activation_19[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.nn.relu_9 (TFOpLambda)      (None, 512)          0           ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_172 (Dropout)          (None, 14, 14, 256)  0           ['max_pooling2d_19[0][0]']       \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 512)          262656      ['tf.nn.relu_9[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 50176)        0           ['dropout_172[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_211 (Dropout)          (None, 512)          0           ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 512)          25690624    ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 512)         1024        ['dropout_211[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.relu_8 (TFOpLambda)      (None, 512)          0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.relu_10 (TFOpLambda)     (None, 512)          0           ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 512)          262656      ['tf.nn.relu_8[0][0]']           \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 512)          262656      ['tf.nn.relu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_173 (Dropout)          (None, 512)          0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_212 (Dropout)          (None, 512)          0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 512)         1024        ['dropout_173[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 512)         1024        ['dropout_212[0][0]']            \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 1024)         0           ['layer_normalization_8[0][0]',  \n",
      "                                                                  'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_213 (Dropout)          (None, 1024)         0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " ITM_Classifier-flickr (Dense)  (None, 2)            2050        ['dropout_213[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 136,750,018\n",
      "Trainable params: 27,266,818\n",
      "Non-trainable params: 109,483,200\n",
      "__________________________________________________________________________________________________\n",
      "TRAINING model\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 224, 224, 32  896         ['image_input[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 224, 224, 32  128        ['conv2d_16[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 224, 224, 32  0           ['batch_normalization_16[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_16 (MaxPooling2D  (None, 112, 112, 32  0          ['activation_16[0][0]']          \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 112, 112, 64  18496       ['max_pooling2d_16[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 112, 112, 64  256        ['conv2d_17[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 112, 112, 64  0           ['batch_normalization_17[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_17 (MaxPooling2D  (None, 56, 56, 64)  0           ['activation_17[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 56, 56, 128)  73856       ['max_pooling2d_17[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 56, 56, 128)  512        ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 56, 56, 128)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_18 (MaxPooling2D  (None, 28, 28, 128)  0          ['activation_18[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " text_input_ids (InputLayer)    [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " text_attention_mask (InputLaye  [(None, 300)]       0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 28, 28, 256)  295168      ['max_pooling2d_18[0][0]']       \n",
      "                                                                                                  \n",
      " tf_bert_model_4 (TFBertModel)  TFBaseModelOutputWi  109482240   ['text_input_ids[0][0]',         \n",
      "                                thPoolingAndCrossAt               'text_attention_mask[0][0]']    \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 300,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 28, 28, 256)  1024       ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4 (Gl  (None, 768)         0           ['tf_bert_model_4[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 28, 28, 256)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 512)          393728      ['global_average_pooling1d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " max_pooling2d_19 (MaxPooling2D  (None, 14, 14, 256)  0          ['activation_19[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.nn.relu_9 (TFOpLambda)      (None, 512)          0           ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_172 (Dropout)          (None, 14, 14, 256)  0           ['max_pooling2d_19[0][0]']       \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 512)          262656      ['tf.nn.relu_9[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 50176)        0           ['dropout_172[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_211 (Dropout)          (None, 512)          0           ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 512)          25690624    ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 512)         1024        ['dropout_211[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.relu_8 (TFOpLambda)      (None, 512)          0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.relu_10 (TFOpLambda)     (None, 512)          0           ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 512)          262656      ['tf.nn.relu_8[0][0]']           \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 512)          262656      ['tf.nn.relu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_173 (Dropout)          (None, 512)          0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_212 (Dropout)          (None, 512)          0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 512)         1024        ['dropout_173[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 512)         1024        ['dropout_212[0][0]']            \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 1024)         0           ['layer_normalization_8[0][0]',  \n",
      "                                                                  'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_213 (Dropout)          (None, 1024)         0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " ITM_Classifier-flickr (Dense)  (None, 2)            2050        ['dropout_213[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 136,750,018\n",
      "Trainable params: 27,266,818\n",
      "Non-trainable params: 109,483,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "1212/1212 [==============================] - 379s 306ms/step - loss: 0.7950 - binary_accuracy: 0.5441 - val_loss: 0.8220 - val_binary_accuracy: 0.5215\n",
      "Epoch 2/10\n",
      "1212/1212 [==============================] - 367s 303ms/step - loss: 0.6821 - binary_accuracy: 0.6050 - val_loss: 0.6821 - val_binary_accuracy: 0.6409\n",
      "Epoch 3/10\n",
      " 805/1212 [==================>...........] - ETA: 1:58 - loss: 0.6565 - binary_accuracy: 0.6280"
     ]
    }
   ],
   "source": [
    "itm = ITM_Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd6c44e6e90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_loss = itm.history.history['val_binary_accuracy']\n",
    "train_loss = itm.history.history['binary_accuracy']\n",
    "print(itm.epochs, len(val_loss))\n",
    "\n",
    "epochs = range(0, itm.epochs)\n",
    "plt.plot(epochs, val_loss, \"b\",\n",
    "         label=\"Validation loss\")\n",
    "plt.plot(epochs, train_loss, \"b+\",\n",
    "         label=\"Train loss\")\n",
    "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'itm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mplot_model(\u001b[43mitm\u001b[49m\u001b[38;5;241m.\u001b[39mclassifier_model,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_name.png\u001b[39m\u001b[38;5;124m'\u001b[39m,show_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'itm' is not defined"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(itm.classifier_model,'img_name.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /home/computing/anaconda3/envs/AML/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/computing/anaconda3/envs/AML/lib/python3.10/site-packages (from pydot) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
