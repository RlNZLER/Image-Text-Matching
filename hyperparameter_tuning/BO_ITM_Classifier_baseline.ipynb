{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\GPU\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "d:\\Conda\\envs\\GPU\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "d:\\Conda\\envs\\GPU\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from official.nlp import optimization\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for loading image and text data\n",
    "\n",
    "class ITM_DataLoader:\n",
    "    BATCH_SIZE = 16\n",
    "    IMAGE_SIZE = (224, 224)\n",
    "    IMAGE_SHAPE = (224, 224, 3)\n",
    "    SENTENCE_EMBEDDING_SHAPE = 384\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    DATA_PATH = \"D:\\_GITHUB_\\Image-Text-Matching\\data\"\n",
    "    IMAGES_PATH = DATA_PATH + \"/images\"\n",
    "    train_data_file = DATA_PATH + \"/flickr8k.TrainImages.txt\"\n",
    "    dev_data_file = DATA_PATH + \"/flickr8k.DevImages.txt\"\n",
    "    test_data_file = DATA_PATH + \"/flickr8k.TestImages.txt\"\n",
    "    sentence_embeddings_file = (DATA_PATH + \"/flickr8k.cmp9137.sentence_transformers.pkl\")\n",
    "    sentence_embeddings = {}\n",
    "    train_ds = None\n",
    "    val_ds = None\n",
    "    test_ds = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sentence_embeddings = self.load_sentence_embeddings()\n",
    "        self.train_ds = self.load_classifier_data(self.train_data_file)\n",
    "        self.val_ds = self.load_classifier_data(self.dev_data_file)\n",
    "        self.test_ds = self.load_classifier_data(self.test_data_file)\n",
    "\n",
    "    def load_sentence_embeddings(self):\n",
    "        with open(self.sentence_embeddings_file, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def process_input(self, img_path, dense_vector, text, label):\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (224, 224))\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        return {'image_input': img, 'text_embedding': dense_vector, 'caption': text, 'file_name': img_path}, label\n",
    "\n",
    "    def load_classifier_data(self, data_files):\n",
    "        print(\"LOADING data from \" + str(data_files))\n",
    "        print(\"=========================================\")\n",
    "        image_data = []\n",
    "        text_data = []\n",
    "        embeddings_data = []\n",
    "        label_data = []\n",
    "\n",
    "        # get image, text, label of image_files\n",
    "        with open(data_files) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.rstrip(\"\\n\")\n",
    "                img_name, text, raw_label = line.split(\"\t\")\n",
    "                img_name = os.path.join(self.IMAGES_PATH, img_name.strip())\n",
    "\n",
    "                # get binary labels from match/no-match answers\n",
    "                label = [1, 0] if raw_label == \"match\" else [0, 1]\n",
    "                # print(\"I=%s T=%s _L=%s L=%s\" % (img_name, text, raw_label, label))\n",
    "\n",
    "                # get sentence embeddings (of textual captions)\n",
    "                text_sentence_embedding = self.sentence_embeddings[text]\n",
    "                text_sentence_embedding = tf.constant(text_sentence_embedding)\n",
    "\n",
    "                image_data.append(img_name)\n",
    "                embeddings_data.append(text_sentence_embedding)\n",
    "                text_data.append(text)\n",
    "                label_data.append(label)\n",
    "\n",
    "        print(\"|image_data|=\" + str(len(image_data)))\n",
    "        print(\"|text_data|=\" + str(len(text_data)))\n",
    "        print(\"|label_data|=\" + str(len(label_data)))\n",
    "\n",
    "        # prepare a tensorflow dataset using the lists generated above\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (image_data, embeddings_data, text_data, label_data)\n",
    "        )\n",
    "        dataset = dataset.shuffle(self.BATCH_SIZE * 8)\n",
    "        dataset = dataset.map(self.process_input, num_parallel_calls=self.AUTOTUNE)\n",
    "        dataset = dataset.batch(self.BATCH_SIZE).prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# Main class for the Image-Text Matching (ITM) task\n",
    "\n",
    "class ITM_Classifier(ITM_DataLoader):\n",
    "    epochs = 10\n",
    "    learning_rate = 3e-5\n",
    "    class_names = {\"match\", \"no-match\"}\n",
    "    num_classes = len(class_names)\n",
    "    classifier_model = None\n",
    "    history = None\n",
    "    classifier_model_name = \"ITM_Classifier-flickr\"\n",
    "\n",
    "    def __init__(self,num_projection_layers=1,projection_dims=128,dropout_rate=0.1,learning_rate=3e-5,):\n",
    "        self.num_projection_layers = num_projection_layers\n",
    "        self.projection_dims = projection_dims\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        super().__init__()\n",
    "        if not self.train_ds:\n",
    "            raise Exception(\"Training dataset not initialized properly.\")\n",
    "        self.build_classifier_model()\n",
    "        self.train_classifier_model()\n",
    "        self.test_classifier_model()\n",
    "\n",
    "    # return learnt feature representations of input data (images)\n",
    "    def create_vision_encoder(\n",
    "        self, num_projection_layers, projection_dims, dropout_rate\n",
    "    ):\n",
    "        img_input = layers.Input(shape=self.IMAGE_SHAPE, name=\"image_input\")\n",
    "        cnn_layer = layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\")(img_input)\n",
    "        cnn_layer = layers.MaxPooling2D()(cnn_layer)\n",
    "        cnn_layer = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(cnn_layer)\n",
    "        cnn_layer = layers.MaxPooling2D()(cnn_layer)\n",
    "        cnn_layer = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(cnn_layer)\n",
    "        cnn_layer = layers.MaxPooling2D()(cnn_layer)\n",
    "        cnn_layer = layers.Dropout(dropout_rate)(cnn_layer)\n",
    "        cnn_layer = layers.Flatten()(cnn_layer)\n",
    "        outputs = self.project_embeddings(\n",
    "            cnn_layer, num_projection_layers, projection_dims, dropout_rate\n",
    "        )\n",
    "        return img_input, outputs\n",
    "\n",
    "    # return learnt feature representations based on dense layers, dropout, and layer normalisation\n",
    "    def project_embeddings(\n",
    "        self, embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    ):\n",
    "        projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "        for _ in range(num_projection_layers):\n",
    "            x = tf.nn.gelu(projected_embeddings)\n",
    "            x = layers.Dense(projection_dims)(x)\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            x = layers.Add()([projected_embeddings, x])\n",
    "            projected_embeddings = layers.LayerNormalization()(x)\n",
    "        return projected_embeddings\n",
    "\n",
    "    # return learnt feature representations of input data (text embeddings in the form of dense vectors)\n",
    "    def create_text_encoder(self, num_projection_layers, projection_dims, dropout_rate):\n",
    "        text_input = keras.Input(\n",
    "            shape=self.SENTENCE_EMBEDDING_SHAPE, name=\"text_embedding\"\n",
    "        )\n",
    "        outputs = self.project_embeddings(\n",
    "            text_input, num_projection_layers, projection_dims, dropout_rate\n",
    "        )\n",
    "        return text_input, outputs\n",
    "\n",
    "    # put together the feature representations above to create the image-text (multimodal) deep learning model\n",
    "    def build_classifier_model(self):\n",
    "        print(f\"BUILDING model\")\n",
    "        img_input, vision_net = self.create_vision_encoder(\n",
    "            num_projection_layers=self.num_projection_layers,\n",
    "            projection_dims=self.projection_dims,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "        )\n",
    "        text_input, text_net = self.create_text_encoder(\n",
    "            num_projection_layers=self.num_projection_layers,\n",
    "            projection_dims=self.projection_dims,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "        )\n",
    "        net = tf.keras.layers.Concatenate(axis=1)([vision_net, text_net])\n",
    "        net = tf.keras.layers.Dropout(self.dropout_rate)(net)\n",
    "        net = tf.keras.layers.Dense(\n",
    "            self.num_classes, activation=\"softmax\", name=self.classifier_model_name\n",
    "        )(net)\n",
    "        self.classifier_model = tf.keras.Model(\n",
    "            inputs=[img_input, text_input], outputs=net\n",
    "        )\n",
    "        self.classifier_model.summary()\n",
    "\n",
    "    def train_classifier_model(self):\n",
    "        print(f\"TRAINING model\")\n",
    "        steps_per_epoch = tf.data.experimental.cardinality(self.train_ds).numpy()\n",
    "        num_train_steps = steps_per_epoch * self.epochs\n",
    "        num_warmup_steps = int(0.2 * num_train_steps)\n",
    "\n",
    "        loss = tf.keras.losses.KLDivergence()\n",
    "        metrics = tf.keras.metrics.BinaryAccuracy()\n",
    "        optimizer = optimization.create_optimizer(\n",
    "            init_lr=self.learning_rate,\n",
    "            num_train_steps=num_train_steps,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            optimizer_type=\"adamw\",\n",
    "        )\n",
    "\n",
    "        self.classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "        # uncomment the next line if you wish to make use of early stopping during training\n",
    "        # callbacks = [tf.keras.callbacks.EarlyStopping(patience=11, restore_best_weights=True)]\n",
    "\n",
    "        self.history = self.classifier_model.fit(\n",
    "            x=self.train_ds, validation_data=self.val_ds, epochs=self.epochs\n",
    "        )  # , callbacks=callbacks)\n",
    "        print(\"model trained!\")\n",
    "\n",
    "    def test_model_for_tuning(self):\n",
    "        print(\"TESTING classifier model for tuning...\")\n",
    "        total_samples = 0\n",
    "        total_correct_predictions = 0\n",
    "\n",
    "        # Iterate over the test dataset\n",
    "        for features, groundtruth in self.test_ds:\n",
    "            predictions = self.classifier_model.predict(\n",
    "                features\n",
    "            )  # Get model predictions\n",
    "            predicted_classes = tf.argmax(predictions, axis=1)\n",
    "            actual_classes = tf.argmax(groundtruth, axis=1)\n",
    "\n",
    "            # Calculate correct predictions\n",
    "            correct_predictions = tf.reduce_sum(\n",
    "                tf.cast(predicted_classes == actual_classes, tf.float32)\n",
    "            )\n",
    "            total_correct_predictions += correct_predictions.numpy()\n",
    "            total_samples += groundtruth.shape[0]\n",
    "\n",
    "            # Optionally print some predictions\n",
    "            if (\n",
    "                random.random() < 0.1\n",
    "            ):  # Roughly 10% chance to print a batch's sample prediction\n",
    "                sample_index = random.randint(0, groundtruth.shape[0] - 1)\n",
    "                caption = features[\"caption\"][sample_index].numpy().decode(\"utf-8\")\n",
    "                match_probability = predictions[sample_index][0]\n",
    "                print(\n",
    "                    f\"Sample Caption: '{caption}', Match Probability: {match_probability:.4f}\"\n",
    "                )\n",
    "\n",
    "        # Calculate and print the overall accuracy\n",
    "        accuracy = total_correct_predictions / total_samples\n",
    "        print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Evaluate using TensorFlow's built-in metrics\n",
    "        loss, tf_accuracy = self.classifier_model.evaluate(self.test_ds)\n",
    "        print(f\"TensorFlow Evaluate Loss: {loss:.4f}, Accuracy: {tf_accuracy:.4f}\")\n",
    "\n",
    "        return tf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    # Define hyperparameters\n",
    "    num_projection_layers = trial.suggest_int('num_projection_layers', 1, 3)\n",
    "    projection_dims = trial.suggest_int('projection_dims', 64, 256)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "\n",
    "    # Initialize the ITM Classifier with the suggested hyperparameters\n",
    "    itm_classifier = ITM_Classifier(\n",
    "        num_projection_layers=num_projection_layers, \n",
    "        projection_dims=projection_dims, \n",
    "        dropout_rate=dropout_rate, \n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    return itm_classifier\n",
    "\n",
    "def objective(trial):\n",
    "    tf.keras.backend.clear_session()  # Clear the TensorFlow graph\n",
    "    itm_classifier = create_model(trial)\n",
    "    accuracy = itm_classifier.test_model_for_tuning()\n",
    "    return 1 - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=20)  # Set the number of trials based on your resource availability\n",
    "\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(f'  Value: {1 - trial.value}')\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'    {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-13 14:45:35,882] A new study created in memory with name: no-name-789e1666-6bc1-437f-b986-2773c54f5416\n",
      "C:\\Users\\Amel Varghese\\AppData\\Local\\Temp\\ipykernel_69836\\2873690747.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "C:\\Users\\Amel Varghese\\AppData\\Local\\Temp\\ipykernel_69836\\2873690747.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING data from D:\\_GITHUB_\\Image-Text-Matching\\data/flickr8k.TrainImages.txt\n",
      "=========================================\n",
      "|image_data|=19386\n",
      "|text_data|=19386\n",
      "|label_data|=19386\n",
      "LOADING data from D:\\_GITHUB_\\Image-Text-Matching\\data/flickr8k.DevImages.txt\n",
      "=========================================\n",
      "|image_data|=1164\n",
      "|text_data|=1164\n",
      "|label_data|=1164\n",
      "LOADING data from D:\\_GITHUB_\\Image-Text-Matching\\data/flickr8k.TestImages.txt\n",
      "=========================================\n",
      "|image_data|=1161\n",
      "|text_data|=1161\n",
      "|label_data|=1161\n",
      "BUILDING model\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 224, 224, 16  448         ['image_input[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 112, 112, 16  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 112, 112, 32  4640        ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 32)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 56, 56, 64)   18496       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 64)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 28, 28, 64)   0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 50176)        0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " text_embedding (InputLayer)    [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 152)          7626904     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 152)          58520       ['text_embedding[0][0]']         \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 152)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 152)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 152)          23256       ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 152)          23256       ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 152)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 152)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 152)          0           ['dense[0][0]',                  \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 152)          0           ['dense_2[0][0]',                \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 152)         304         ['add[0][0]']                    \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 152)         304         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 304)          0           ['layer_normalization[0][0]',    \n",
      "                                                                  'layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 304)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " ITM_Classifier-flickr (Dense)  (None, 2)            610         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,756,738\n",
      "Trainable params: 7,756,738\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "TRAINING model\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\functional.py:637: UserWarning: Input dict contained keys ['caption', 'file_name'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
